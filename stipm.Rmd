---
title: "Willapa Bay natural coho forecast"
author: "Dan.Auerbach@dfw.wa.gov, Thomas.Buehrens@dfw.wa.gov, and Neala.Kendall@dfw.wa.gov" 
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
output: 
  wdfwTemplates::wdfw_html_format

---

# Summary

The Pacific Fishery Management Council (PFMC) reviews annual catch limits (ACLs, equivalent to acceptable biological catch or ABC) for the Willapa Bay natural coho [stock aggregate](https://www.pcouncil.org/documents/2021/02/e-3-situation-summary-review-of-2020-fisheries-and-summary-of-2021-stock-forecasts.pdf/). As part of this process, the PFMC's Scientific and Statistical Committee ([SSC](https://www.pcouncil.org/navigating-the-council/membership-groups-and-staff/advisory-groups/scientific-and-statistical-committee-ssc/)) evaluates the methods used to develop forecasts.

This script presents a new framework that the Washington Department of Fish and Wildlife (WDFW) proposes to use for preseason forecasts of Willapa Bay natural coho. The proposed approach leverages multiple data sources while rigorously depicting process and observation uncertainty, and it examines predictive skill with measures based on one-ahead tests to replicate real-world data conditions. As a central element of this framework, the code below updates and extends the [original implementation](https://github.com/lukasdefilippo/ST-IPM) of [DeFilippo et al.'s (2021)](https://www.sciencedirect.com/science/article/pii/S0165783621001429) "Spatiotemporal Integrated Population Model" (ST-IPM) of Washington state natural origin coho (*O. kisutch*) returns. This peer-reviewed, state-of-the-science ST-IPM is built around existing management units in the Coho Fishery Regulation and Assessment Model ([coho FRAM](https://framverse.github.io/fram_doc/calcs_data_coho.html)) and is well integrated with the data compilation and reporting workflows and timelines of PFMC (and the Coho Technical Committee of the Pacific Salmon Commission). For Willapa Bay (WB), the proposed approach incorporates both WB-specific natural origin return data and WB hatchery origin marine survival, alongside smolt outmigrant, survival and return data from the Chehalis Basin and other WA coastal stocks.

Estimated returns of WB natural coho (i.e., spawning escapement plus fishery catches) have shown substantial annual and longer-term fluctuations, posing a challenge to accurate preseason forecasts based on any method. For example, over the last decade, the population has exhibited both a major declining trend (from an average of ~108K in 2009-10 to ~18K in 2018-19) as well as dramatic year on year reversals (from ~96K in 2014 to ~19K in 2015). This difficult context heightens the importance of applying forecast methods that provide fishery managers with more than a single point estimate, and that can supply additional information to quantify relative risks of alternative decisions. 


# Setup

The following code readies the R environment (`r sessionInfo()$R.version$version.string`), calling necessary packages and loading an initial, complete dataset.

```{r setup, results = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, results = FALSE, warning = FALSE, message = FALSE, fig.width = 9, fig.height = 9)

library("tidyverse")
library("gt")
library("odbc"); library("DBI")
library("rstan")
options(mc.cores = 10) #12 on DA machine
rstan_options(auto_write = TRUE)
theme_set(theme_light())


fp <- list(
  # coho_file = 'Coho data_3-30-20.csv',
  # cwt_file = 'CWT_FRAM_Matches_complete20200303.csv',
  # stream_file = 'Coho_KM_3.31.2020_2.csv',
  data_fram = "data/postseason_fram_spwn_hvst_ests.csv",
  data_cwt = "data/rmis_cwts.csv",
  data_smolt = "data/smolt_outmigrants.csv",
  data_full = "data/fram_cwt_smolt_fulljoin.csv"
  )

# ## dropped FRAM StockID 13 and 43, original pop_ids 10 and 24
# # fram_stocks <- c(105,93,89,45,51,55,75,81,61,23,149,63,107,115,111,157,97,135,153,101,69,1,85,139,131,127,145,11,17,59,35,29,117,161)
# 
# pop_meta <- readxl::read_excel("pop_meta.xlsx") |>
#   dplyr::select(pop_id:hat)
# #deleted and reindexed pop in file, no longer need: |> dplyr::filter(StockID != 13, StockID != 43)

coho_data_tbl <- readr::read_csv(fp$data_full) #can explore coercing pre-1998 FRAM to NA while including older MS & smolt 

sf_coord <- coho_data_tbl |> 
  distinct(StockID, StockLongName, pop_id, pop, lon, lat, hab_km) |> 
  sf::st_as_sf(coords = c("lon", 'lat'), crs = sf::st_crs("+proj=longlat +datum=WGS84")) |> 
  sf::st_transform(crs = sf::st_crs("+proj=utm +zone=10T ellps=WGS84"))

##load("stipm.Rdata")

```

# Data

The analysis relies on per-stock escapement and harvest estimates compiled in post-season coho FRAM runs, as well as CWT-based marine survival estimates derived from RMIS, and smolt trap outmigrant estimates compiled by WDFW. The full set of stocks included in the dataset are shown below.

```{r population_map}
## figure out an enlarged bounding box
# coho_data_tbl |> 
#   distinct(StockID, StockLongName, pop_id, pop, lon, lat, hab_km) |> 
#   sf::st_as_sf(coords = c("lon", 'lat'), crs = sf::st_crs("+proj=longlat +datum=WGS84")) |>
#   sf::st_bbox()

sf::st_bbox(c(xmin = -126, ymin = 46, xmax = -120, ymax = 50), crs = sf::st_crs("+proj=longlat +datum=WGS84")) |> 
  basemaps::basemap_ggplot(map_service = "esri", map_type = "world_imagery") +
  geom_sf(data = sf::st_transform(sf_coord, crs = sf::st_crs(3857)), color = "orange") +
  ggrepel::geom_text_repel(
    data = sf_coord |> mutate(pop = str_remove(pop, "Natural|Wild")) |> sf::st_transform(crs = sf::st_crs(3857)),
    aes(label = paste(StockID, pop, sep = ":"), geometry = geometry),
    stat = "sf_coordinates", color = "orange") +
  coord_sf()

# #simple greyscale options without cleaned up labels
# coho_data_tbl |> 
#   distinct(StockID, StockLongName, pop_id, pop, lon, lat, hab_km) |> 
#   ggplot(aes(lon, lat)) + 
#   geom_point(aes(size = hab_km)) + 
#   geom_text(aes(label = paste(StockID, pop, sep = ":")), check_overlap = T) +
#   #borders("county", regions = "washington") +
#   borders("state", regions = "washington") +
#   coord_map("conic", lat0 = 46, xlim = c(-125.5, -121))
# ggplot() +
#   geom_sf(data = USAboundaries::us_states(states = "WA") |>
#             sf::st_transform(crs = sf::st_crs(sf_coord)) |>
#             sf::st_crop(xmin = 376000, ymin = 5172000, xmax = 560000, ymax = 5404000), #sf::st_bbox(sf_coord)
#           aes(label = name)) +
#   geom_sf(data = sf_coord, aes(size = hab_km)) +
#   geom_sf_text(data = sf_coord, aes(label = paste(StockID, pop, sep = ":")), check_overlap = T)

```


## FRAM postseason estimates of spawning escapement and harvest mortality

Coho FRAM tracks age 3 fish across 5 time steps corresponding to a calendar year. It includes unmarked and marked units of both natural and hatchery stocks. 

Data in the best available database extend to 1986, but note that values prior to 1998 are of unknown origin.

The following code chunks demonstrate a reproducible extraction of the necessary datasets. Note that the `read_` functions require a database maintained by PSC CoTC.

```{r fram_readers, eval=FALSE}
read_coho_backwards <- function (db, runs = NULL, stocks = NULL) {

  db_con <- DBI::dbConnect(drv = odbc::odbc(),
    .connection_string = paste0("Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=", db, ";"))

  #lazy, full table then reduce as specified
  bk <- dplyr::tbl(db_con, "BackwardsFRAM") |> dplyr::select(RunID, StockID, escp = TargetEscAge3)
  if (!is.null(runs)) { bk <- dplyr::filter(bk, RunID %in% runs) }
  if (!is.null(stocks)) { bk <- dplyr::filter(bk, StockID %in% stocks) }

  #associate metainfo and pull
  bk <- bk |> 
    dplyr::left_join(
      dplyr::tbl(db_con, "RunID") |>  dplyr::select(RunID, RunYear, RunName), 
      by = "RunID") |>
    dplyr::left_join(
      dplyr::tbl(db_con, "Stock") |> dplyr::filter(Species == "COHO") |> dplyr::select(StockID, StockLongName),
      by = "StockID") |>
    dplyr::collect() |> 
    dplyr::arrange(RunYear, StockID)

  DBI::dbDisconnect(db_con)
  
  return(bk)
}

read_coho_escapement <- function (db, runs = NULL, stocks = NULL) {

  db_con <- DBI::dbConnect(drv = odbc::odbc(),
    .connection_string = paste0("Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=", db, ";"))
  #coho is already only age 3 in TimeStep 5
  #lazy, full table then reduce as specified
  escp <- dplyr::tbl(db_con, "Escapement") |> dplyr::select(RunID, StockID, escp = Escapement)
  if (!is.null(runs)) { escp <- dplyr::filter(escp, RunID %in% runs) }
  if (!is.null(stocks)) { escp <- dplyr::filter(escp, StockID %in% stocks) }

  #associate metainfo and pull
  escp <- escp |> 
    dplyr::left_join(
      dplyr::tbl(db_con, "RunID") |>  dplyr::select(RunID, RunYear, RunName), 
      by = "RunID") |>
    dplyr::left_join(
      dplyr::tbl(db_con, "Stock") |> dplyr::filter(Species == "COHO") |> dplyr::select(StockID, StockLongName),
      by = "StockID") |>
    dplyr::collect() |> 
    dplyr::arrange(RunYear, StockID)

  DBI::dbDisconnect(db_con)
  
  return(escp)
}

read_coho_mort <- function (db, runs = NULL, stocks = NULL) {
  
  db_con <- DBI::dbConnect(drv = odbc::odbc(),
    .connection_string = paste0("Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=", db, ";"))
  
  #lazy, full table then reduce as specified
  m <- dplyr::tbl(db_con, "Mortality") |> dplyr::select(-PrimaryKey)
  if (!is.null(runs)) { m <- dplyr::filter(m, RunID %in% runs) }
  if (!is.null(stocks)) { m <- dplyr::filter(m, StockID %in% stocks) }

  #associate metainfo and pull
  m <- m |> 
    dplyr::left_join(
      dplyr::tbl(db_con, "RunID") |>  dplyr::select(RunID, RunYear, RunName), 
      by = "RunID") |>
    dplyr::left_join(
      dplyr::tbl(db_con, "Stock") |> dplyr::filter(Species == "COHO") |> dplyr::select(StockID, StockLongName),
      by = "StockID") |>
    dplyr::left_join(
      dplyr::tbl(db_con, "Fishery") |> dplyr::filter(Species == "COHO") |> dplyr::select(FisheryID, FisheryName), 
      by = "FisheryID") |> 
    dplyr::collect() |> 
    dplyr::arrange(RunYear, FisheryID, TimeStep, StockID)

  DBI::dbDisconnect(db_con)
  
  return(m)
}
```

```{r read_and_export_fram_tables, eval=FALSE}
#the relevant data are queried and re-exported as csv
#stays similar to original and allows greater reproducibility/portability
mdb <- "O:/code/coho_fram_validation/PSC_CoTC_PostSeason_CohoFRAMDB_thru2019_021021.mdb"
#appears to be a mysterious RunID 3 in the Escapement and Mortality tables but not RunID table...
#aggregate mortality across timesteps for all sources to "estimated total harvest related impacts"
fram <- full_join(
  read_coho_escapement(mdb, stocks = pop_meta$FRAM_StockID) |> 
    filter(!is.na(RunYear))
  ,
  read_coho_mort(mdb, stocks = pop_meta$FRAM_StockID) |>
    filter(!is.na(RunYear)) |> 
    mutate(mort = LandedCatch + NonRetention + Shaker + DropOff + MSFLandedCatch + MSFNonRetention + MSFShaker + MSFDropOff) |> 
    group_by(RunYear, StockID, StockLongName) |> 
    summarise(mort = sum(mort), .groups = "drop")
  ,
  by = c("RunYear", "StockID", "StockLongName")
  ) |> 
  select(year = RunYear, StockID, StockLongName, spwn = escp, hvst = mort)

#fram |> filter(StockID == 161) |> print(n = 100)

write_csv(fram, "data/postseason_fram_spwn_hvst_ests.csv")

```

## Smolt outmigrant estimates

Smolt trap estimates are included from a dataset maintained by Marisa Litz (WDFW).

```{r request_to_ML, eval=FALSE}
#tbl_coho |> filter(!is.na(`Smolt Abundance`), year >= 1986) |> count(pop)

smolt_used <- stan_data_filter(coho_data_tbl, data_year_min = 1986, data_year_max = 2021, lag_spwn = 1, lag_hvst = 1) |> 
  getElement("smolt") 
#writexl::write_xlsx(smolt_used, "O:/code/coho/forecast_wb/smolt_outmig_to_update.xlsx")

smolt_used |> group_by(pop) |> summarise(ymin = min(year), ymax = max(year))

#original script drops numerous smolt outmigrant series that apparently could not be well reconciled to FRAM units
smolt_full_orig <- readr::read_csv(fp$coho_file, show_col_types = FALSE) |> 
  mutate(
    pop = case_when(
      !is.na(`SaSI Population`) ~ `SaSI Population`,
      is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Managment Unit (FRAM)`,
      is.na(`SaSI Population`) & !is.na(SubPopulation) ~ SubPopulation
    )
  ) |> 
  select(1:8, pop) |> 
  #count(Smolt_Abundance_Matches_FRAM)
  #count(Smolt_Abunce_Matches_SASI_but_not_FRAM)
  filter(!is.na(`Smolt Abundance`), `Calendar Year` >= 1986)
#see "drop_pop"
smolt_used |> count(pop, smolt_ocn_surv_pop) #9 units
smolt_full_orig |> count(`Managment Unit (FRAM)`, `Smolt Abundance Population`, pop) |> print(n = 50) #23 units

left_join(
  smolt_used |> count(pop, smolt_ocn_surv_pop)
  ,
  smolt_full_orig |> count(`Managment Unit (FRAM)`, `Smolt Abundance Population`, pop) #|> print(n = 50)
  , by = c("pop" = "Managment Unit (FRAM)")
  ) |> 
  filter(!(pop.y %in% drop_pop)) |> 
  select(-contains("."))

```

```{r smolt_update_checks_export, eval=FALSE}

smolt <- readxl::read_excel("O:/code/coho/forecast_wb/Smolt Time Series.xlsx", na = "NA") |> 
  select(
    year = OEY,
    Chehalis_149 = Chehalis,
    Deschutes_63 = Deschutes,
    Dungeness_107 = Dungeness,
    Green_97 = Green,
    Nisqually_69 = Nisqually,
    Nooksack_1 = Nooksack,
    Queets_139 = Queets.Clear,
    Skagit_17 = Skagit,
    Snohomish_35 = Snohomish) |> 
  filter(year >= 1986) |> 
  pivot_longer(cols = -year, names_to = "smoltpop_StockID", values_to = "smolt_update") |> 
  separate(smoltpop_StockID, into = c("smoltpop","StockID"), sep = "_") |> 
  mutate(StockID = as.numeric(StockID)) |> 
  filter(!is.na(smolt_update)) #excel wide format has complete cases, so reduce to stock-years with data


full_join(
  smolt_used |> select(pop_id, pop, StockID, year, smolt_orig = smolt)
  ,
  smolt
  ,
  by = c("StockID", "year")
  ) |> #214
  mutate(
    smolt = if_else(is.na(smolt_update), smolt_orig, smolt_update)
  ) |> 
  #filter(is.na(smolt_orig)) #16 updated values for 2018-2020
  #filter(round(smolt_orig) != round(smolt_update)) #1991 Chehalis does not match
  #filter(is.na(smolt_update)) #prior year vals not present in update, 3 Chehalis and 1 Nooksack
  select(StockID, year, smolt) |> 
  write_csv("data/smolt_outmigrants.csv")

```

## Marine survival CWT releases and recoveries

Estimates of marine survival are included via records of coded wire tagged fish releases and recoveries. This dataset was updated by Neala Kendall and Ty Garber, following fix of RMIS release count error for tagged wild units. (Note the original dataset was unaffected because 1) error not present in hatchery units 2) previous wild unit values drawn from underlying pre-RMIS data.)

```{r cwt_rec_rel_update_checks_export, eval=FALSE}
# #strings match and all 15 units used in analysis are present
# #reversing order of terms shows the 3 unused dataset units: Baker H, Minter Crk H and Satsop H
# setdiff(
#   filter(pop_meta, !is.na(smolt_ocn_surv_pop)) |> distinct(smolt_ocn_surv_pop),
#   readxl::read_excel("O:/code/coho/forecast_wb/coho SARs_to extend.xlsx",
#     sheet = "CWT_FRAM_Matches_complete202108") |>
#     distinct(smolt_ocn_surv_pop = `Smolt Ocean Survival Population`)
# )

cwt <- readxl::read_excel(
  "O:/code/coho/forecast_wb/coho SARs_to extend.xlsx", 
  sheet = "CWT_FRAM_Matches_complete202108"
  ) |> 
  select(
    smolt_ocn_surv_pop = `Smolt Ocean Survival Population`,
    pop = `Managment Unit (FRAM)`,
    year = `Calendar Year`, est_n_rec = Fishery_Plus_Escapement, est_n_rel = Release_No
  ) |> 
  left_join(
    pop_meta |> mutate(pop_id = as.numeric(factor(pop))),
    by = c("pop", "smolt_ocn_surv_pop")
    ) |> 
  #filter(is.na(pop_id)) |> print(n = 100)
  #filter(str_detect(pop, "Area 13")) |> print(n = 100)
  filter(!is.na(pop_id)) |> 
  select(StockID, pop_id, pop, smolt_ocn_surv_pop, year, est_n_rec, est_n_rel)

# #compare against prior
# tbl_coho |> 
#   filter(pop_id == 36, between(year, 1986, 2020)) |> 
#   select(pop_id, pop, `Smolt Ocean Survival Population`, year, Fishery_Plus_Escapement, Release_No) |> print(n = 50)

# full_join(
#   coho_data_tbl |> select(pop_id, pop, smolt_ocn_surv_pop, year, est_n_rec, est_n_rel)
#   ,
#   cwt
#   ,
#   by = c("pop_id", "pop", "smolt_ocn_surv_pop", "year"), suffix = c("_orig", "")
#   ) |>
#   #filter(pop_id == 34, between(year, 1986, 2020)) |> print(n = 50)
#   mutate(
#     d_rec = est_n_rec - est_n_rec_orig,
#     d_rel = est_n_rel - est_n_rel_orig
#   ) |>
#   filter(
#     !is.na(d_rec), !is.na(d_rel),
#     (d_rec != 0 | d_rel != 0)) |>
#   select(pop_id:year, contains("_rel"), contains("_rec")) |> #print(n = 200)
#   writexl::write_xlsx("O:/code/coho/forecast_wb/coho_SARs_to_extend_diffs.xlsx")

cwt |> 
  filter(year >= 1986) |>
  select(StockID, year, est_n_rec, est_n_rel) |> 
  write_csv("data/rmis_cwts.csv")

```

## Combined into full dataset

This first chunk can/shoud be deleted on finalization; reproduces original data object in a tidy pipe.

```{r tbl_coho_full, eval=FALSE}
# #still retaining for now to avoid having to root through commits for any questions
# 
# drop_pop <- c("Queets", "Clearwater", 'Bell Creek', 'Johnson Creek', 'Jimmy Come Lately Creek',
#               'Deep Creek', 'McDonald Creek', 'Siebert Creek', 'Salt Creek', 'Discovery Bay',
#               'East Twin Creek', 'West Twin Creek', 'Northeast Hood Canal')
# 
# drop_cwt_surv <- c("Minter Crk H", "Baker H", "Satsop H") 
# 
# tbl_coho <- readr::read_csv(fp$coho_file, show_col_types = FALSE) |> #1709x21
#   mutate(
#     pop = case_when(
#       !is.na(`SaSI Population`) ~ `SaSI Population`,
#       is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Managment Unit (FRAM)`,
#       is.na(`SaSI Population`) & !is.na(SubPopulation) ~ SubPopulation
#     ),
#     spwn = case_when(
#       !is.na(`SaSI Population`) ~ `SASI Natural Origin Abundance`,
#       is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Age 3 Escapement (FRAM)`,
#       is.na(`SaSI Population`) & !is.na(SubPopulation) ~ `SupPopulation Escapement`
#     ),
#     spwn = if_else(pop=="Discovery Bay", `SASI CompositeOrigin Abundance`, abs(spwn)) #corrects negative val for 2005 A12A Wild, orig L152
#     ,
#     hvst = spwn / (1/`Harvest (% FRAM)` - 1) 
#   ) |> 
#   filter( !(pop %in% drop_pop) ) |> 
#   select(`Calendar Year`, `Smolt Abundance`, Latitude, Longitude, pop, spwn, hvst) |> 
#   inner_join(
#     readr::read_csv(fp$stream_file, show_col_types = FALSE) |> 
#       filter(Population != "", !is.na(Population)) |> #no NAs in the current csv, but left for now
#       select(pop = Population, KM)
#     ,
#     by = "pop") |> 
#   full_join(
#     readr::read_csv(fp$cwt_file, show_col_types = FALSE) |> 
#       mutate(
#         hat = if_else(stringr::str_detect(`Smolt Ocean Survival Population`, " H$"), 1, 0)
#       ) |> 
#       filter( !(`Smolt Ocean Survival Population` %in% drop_cwt_surv)) |> 
#       select(pop = `Managment Unit (FRAM)`, `Calendar Year`, `Smolt Ocean Survival Population`, Fishery_Plus_Escapement, Release_No, hat)
#     ,
#     by = c("pop", "Calendar Year")) |> 
#   filter(`Calendar Year` > 1985) |> 
#   bind_rows(
#     tibble(`Calendar Year` = 2013, yr = 28, pop = "Area 7-7A Independent Wild"),
#     tibble(`Calendar Year` = 2004, yr = 19, pop = "Port Gamble Bay Wild"),
#     tibble(`Calendar Year` = 2000:2001, yr = 15:16, pop = "Grays Harbor Miscellaneous Wild")
#   ) |>
#   left_join(
#     bind_rows(
#       tibble(pop = "Green River Wild", Long = -122.2145, Lat = 47.3519),
#       tibble(pop = "Area 10E Miscellaneous Wild", Long = -122.8242, Lat = 47.5896)
#     )
#     , by = "pop") |>
#   mutate(
#     Longitude = if_else(is.na(Long), Longitude, Long), Long = NULL,
#     Latitude = if_else(is.na(Lat), Latitude, Lat), Lat = NULL,
#     Basin = if_else(Longitude < -123.80, 0, 1),
#     `Smolt Abundance` = if_else(pop == "Puyallup River Wild", NA_real_, `Smolt Abundance`),
#     hvst = if_else(stringr::str_detect(pop, "Quillayute River") & `Calendar Year` <= 1987, NA_real_, hvst),
#     pop_id = as.numeric(factor(pop)),
#     yr = `Calendar Year` - min(`Calendar Year`) + 1
#   ) |> 
#   rename(year = `Calendar Year`) |> 
#   arrange(pop)
# 
# sf_coord <- tbl_coho |> 
#   group_by(pop, pop_id) |> 
#   summarise(long = median(Longitude, na.rm = T), lat = median(Latitude, na.rm = T), .groups = "drop") |> 
#   sf::st_as_sf(coords = c("long", 'lat'), crs = sf::st_crs("+proj=longlat +datum=WGS84")) |> 
#   sf::st_transform(crs = sf::st_crs("+proj=utm +zone=10T ellps=WGS84"))

```

```{r updating_full_dataset, eval=FALSE}
#Note the original negative escapement in A12A Wild in 2005 is due to an error in the FRAM database
#so still needs correcting: spwn is coerced to 0 here, but harvest is unaffected b/c drawn directly from Mortality
#note also Grays Hbr Misc Wild missing 2000 & 2001 in FRAM mdb and Area 13A Miscellaneous Wild missing 1996
#both fixed with simple insertion of NAs

#here the meta identifiers are (expanding left) joined to the FRAM data
#after these have been full joined to full joined CWT and smolt data

coho_data_tbl <- pop_meta |>
  full_join(
    full_join( #FRAM + (CWT + smolt)
      read_csv(fp$data_fram) |>
        select(-StockLongName) |>
        bind_rows(tibble(year = c(1996, 2000:2001), StockID = c(81, 157, 157))) |> 
        mutate(
          spwn = if_else(spwn < 0, 0.01, spwn),
          rtrn = spwn + hvst
        ) |>  
        arrange(StockID, year) #1156, 1986:2019
      ,
      full_join(
        read_csv(fp$data_cwt), #396
        read_csv(fp$data_smolt), #214 
        by = c("StockID", "year")
        )
      ,
      by = c("StockID", "year")
      )
    ,
    by = c("StockID"))

write_csv(coho_data_tbl, "data/fram_cwt_smolt_fulljoin.csv")

# coho_data_tbl |> count(StockID, pop_id, pop) |> print(n = 100)
# 
# #WB has FRAM through 2019, hatchery-based rel/rec through 2018, no smolt
# coho_data_tbl |> filter(StockID == 161) |> print(n = 50)
# 
# #Chehalis has the Bingham Crk numbers
# coho_data_tbl |> filter(StockID == 149) |> print(n = 50)
# 
# #Skagit is an example of "max data": FRAM to 2019, CWT to 2018, smolt to 2020
# coho_data_tbl |> filter(StockID == 17) |> print(n = 50)

```

\

# Model fitting

Models are first fit to progressive subsets of the data filtered by year in order to gauge *as applied* forecast skill. Rather than a "sliding window" (e.g., the prior 10 years stepped forward each year), the dataset is "stretched" from a fixed starting year. Although older observations may have less immediate relevance to current conditions, this approach allows the MCMC sampler to explore a larger parameter space for posterior distributions. Future work could compare skill with a sliding alternative to test the hypothesis that a more constrained but more recent span of years could improve prediction.

In practice, the data available to forecast a given year return may be limited to observations from one, two or even three years prior. These lags can vary by data type and source.

For example, a 2022 preseason forecast, made in January of 2022, would likely have available:
  - Smolt outmigrant abundances from 2020 or 2021
  - CWT recoveries & releases (informing marine survival) from 2019 or 2020; from 2017 brood, released in 2019 with recoveries in 2020
  - FRAM-based estimates of harvest and escapement (informing annual total return) from 2019 or 2020
    - 2019 values are complete from the previous preseason planning process
    - 2020 estimates exist and are being compiled for Jan/Feb CoTC post-season runs
    - preliminary 2021 escapement estimates may be available, but will still be in review and undergoing regional QAQC
    
Accordingly, the process here examines likely dataset scenarios for a forecast targeting year $y$:
  - STIPM: $smolt_{y-1}$, $MS_{y-2}$, $spwn_{y-2}, hvst_{y-2}$ or $spwn_{y-3}, hvst_{y-3}$
  - AR1: $spwn_{y-2}, hvst_{y-2}$ or $spwn_{y-3}, hvst_{y-3}$

## `stan` functions

A helper function trims years from the complete dataset according to arguments controlling the data-type-specific lag from a data `data_year_max` corresponding to the year before the desired forecast year (i.e., a `data_year_max` of 2021 designates the most recent possible data for the 2022 forecast). 

```{r stan_data_filter}
#trim years then declare separate intermediaries
#"data_year_max" is "last/max year of available data", so desired year_pred-1
#lags allow for flexible OAT configurations with additional trimming
#but can be zeroed to allow all available data
#for a 2022 forecast in adult_pred vectors from stan
#data_year_max is 2021, s.t. firmly available postseason FRAM is 2019 
stan_data_filter <- function(
  full, #complete dataset
  data_year_min, #starting anchor year
  #in STIPM, adult_pred[data_year_max+1] is a total return from smolt[data_year_max]*surv[data_year_max], with adult_est[data_year_max] as the spawning escapement
  #in AR1, adult_est and adult_pred are total return, and adult_pred[year_pred] is same as last year of adult_est b/c passing n_year+1 relative to STIPM
  data_year_max,
  lag_smolt = 0, #n-extra-years trimmed from smolt abundances 
  lag_MS = 1, #n-extra-years trimmed from release and recovery data
  lag_spwn = 2, #n-extra-years trimmed from FRAM escapement
  lag_hvst = 2 #n-extra-years trimmed from FRAM harvest
  ){
  
  full_ymin_ymax = filter(full, between(year, data_year_min, data_year_max)) |> 
    mutate(yr = year - min(year) + 1) #reindexing for stan
  
  stan_data = list(
    y_min_max = data_year_min:data_year_max,
    smolt = filter(full_ymin_ymax, !is.na(smolt), year <= data_year_max - lag_smolt),
    MS = filter(full_ymin_ymax, !is.na(est_n_rec), year <= data_year_max - lag_MS),
    spwn = filter(full_ymin_ymax, !is.na(spwn), year <= data_year_max - lag_spwn),
    hvst = filter(full_ymin_ymax, !is.na(hvst), year <= data_year_max - lag_hvst),
    rtrn = filter(full_ymin_ymax, !is.na(rtrn), year <= data_year_max - max(c(lag_spwn, lag_hvst)))
  )
  
  return(stan_data)
}

# #tests, random start year
# stan_data_filter(coho_data_tbl, 2004, 2006) |> map(tail) #predicting 2007, with defaults only get FRAM through 2004
# stan_data_filter(coho_data_tbl, 2004, 2007) |> map(tail) #predicting 2008, defaults give FRAM 2005

```

Next, wrappers to the `rstan::stan` function facilitate repeated fitting with consistent control parameters and input data lists.

```{r ar1_functions}
stan_ar1 <- function(stan_data, n_iter = 200, n_chain = 2){
  #note following orig naming convention where "tot" refers to "total return" = spwn + hvst = rtrn, as estimated from FRAM
  stan_fit <- stan(
    file = 'LD_coho_forecast_AR_ind_2_v2.stan',
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.999, max_treedepth = 12),
    data = list(
      n_year = length(stan_data$y_min_max) + 1, #sets year of adult_pred and year-dim of adult_est 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      n_pop_tot = length(unique(stan_data$rtrn$pop_id)), #Number of populations with return data
      pop_tot = unique(stan_data$rtrn$pop_id), #Which populations possess return data
      n_tot = length(stan_data$rtrn$rtrn), #Length of the return data vectors
      tot_dat = stan_data$rtrn$rtrn,  #Vectors of all return data across all populations
      tot_true = stan_data$rtrn$yr, #Vectors of the indices identifying which years are those with non-NA data for the return data
      #Paired vectors of slice points indicating the beginning, and end of the data for a particular population
      slice_tot_start = stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_tot_end = stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  
  return(stan_fit)
}

```

```{r stipm_functions}
stan_stipm <- function(stan_data, n_iter = 200, n_chain = 2){
  #note n_year does not include "+1" of AR1 since adult_pred is calc'd separately as
  # smolt[n_year]*surv[n_year], thereby giving adult run size (spwn+hvst) in n_year+1
  #stan_data$y_min_max accordingly is first:last_year_of_data
  stan_fit <- stan(
    file = 'LD_coho_forecast_6_2_4.stan', 
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.99, max_treedepth = 10.25),
    data = list(
      n_year = length(stan_data$y_min_max), 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      u = matrix(1, nrow = 1, ncol = length(unique(stan_data$spwn$pop_id))),
      dist = units::drop_units(sf::st_distance(sf_coord)/10000), #values are identical
      
      pop_smolt = unique(stan_data$smolt$pop_id), #pop_ids with smolt data 
      n_pop_smolt = length(unique(stan_data$smolt$pop_id)),
      smolt_true = stan_data$smolt$yr,
      smolt_dat = stan_data$smolt$smolt,
      n_smolt = nrow(stan_data$smolt),
      
      pop_esc = unique(stan_data$spwn$pop_id),  # pop_ids with escapement data
      n_pop_esc = length(unique(stan_data$spwn$pop_id)),
      esc_true = stan_data$spwn$yr,
      esc_dat = stan_data$spwn$spwn,
      n_esc = nrow(stan_data$spwn),
      
      pop_catch = unique(stan_data$hvst$pop_id), #pop_ids with harvest data
      n_pop_catch = length(unique(stan_data$hvst$pop_id)),
      harvest_true = stan_data$hvst$yr,
      harvest_dat = stan_data$hvst$hvst,
      n_harvest = nrow(stan_data$hvst),
      
      pop_MS = unique(stan_data$MS$pop_id), #pop_ids with marine survival data
      n_pop_MS = length(unique(stan_data$MS$pop_id)),
      MS_true = stan_data$MS$yr, #orig uses Fishery_Plus_Escapement rather than Release_No to filter...
      MS_dat_x = stan_data$MS$est_n_rec |> round() |> as.integer(), #stan expects integer; previously labeled Fishery_Plus_Escapement
      MS_dat_N = stan_data$MS$est_n_rel |> round() |> as.integer(), #previously labeled Release_No
      n_MS = nrow(stan_data$MS),
      
      stream_dist = stan_data$spwn |> distinct(pop, hab_km) |> pluck("hab_km"),
      
      sigma_esc = 0.2,
      
      n_hatchery = filter(stan_data$MS, hat == 1) |> distinct(pop_id) |> nrow(),
      hatchery = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat > 0, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"),
      wild = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat < 1, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"), 
      
      slice_smolt_start = stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_smolt_end = stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_esc_start = stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_esc_end = stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_harvest_start = stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_harvest_end = stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_MS_start = stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_MS_end = stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  return(stan_fit)
}
```

## One-ahead *post-hoc* forecast

The one-at-a-time model fitting proceeds by iterating through a vector of "max data years", predicting the following year by first filtering down a subset of the estimation data, then calling the stan models, and then extracting diagnostic and posterior results.  

```{r oat, eval=FALSE}
#intermediary single year output is saved out within map() in case of loop disruptions
#a dataset data_year_max of 2014, predicting 2015, with default lags of 2 gives FRAM/spwn through 2012
#but with "compiling" lags of 1 gives FRAM/spwn through 2013

oat_wrap <- function(iter, #3K 
                     chain, #4
                     oat_data_year_min, #1998,
                     oat_years_max, #2008:2018, #max data year, predicting 2009:2019
                     lag_spwn, lag_hvst, lag_MS, lag_smolt #deducted from data year
                     ){
  #build a list named by max data year 
  oat <- set_names(oat_years_max) |> 
    map(function(x) {
      print(paste("max data year", x, "predicting", x+1))
      stan_data_list <- stan_data_filter(coho_data_tbl, data_year_min = oat_data_year_min, data_year_max = x, lag_spwn = lag_spwn, lag_hvst = lag_hvst, lag_MS = lag_MS, lag_smolt = lag_smolt) 
      print(stan_data_list$y_min_max)
      print(c("smolt", range(stan_data_list$smolt$year)))
      print(c("MS", range(stan_data_list$MS$year)))
      print(c("spwn", range(stan_data_list$spwn$year)))
      print(c("hvst", range(stan_data_list$hvst$year)))
      print(c("rtrn", range(stan_data_list$rtrn$year)))

      print(paste("AR1 start", Sys.time()))

      fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall AR1 stan data arg n_year+1 relative to STIPM gives adult_pred in desired year
      fit_ar1_pred <- summary(fit_ar1, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "ar1",
          n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      print(paste("STIPM start", Sys.time()))

      fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall stipm adult_pred is smolt[n_year]*surv[n_year] to give adult run size (spwn+hvst) n_year+1
      fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "stipm",
          n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      pred_out <- bind_rows(fit_ar1_pred, fit_stipm_pred)
      saveRDS(pred_out, paste0("oat_fits/oat_pred_",x+1,"_lags_spwn",lag_spwn,"_hvst",lag_hvst,"_MS",lag_MS,"_smolt",lag_smolt,".rds"))
      return(pred_out) #list element for max-data-year-x 
    })
  return(oat)
}

#predicting 2009:2019; renaming objects with lags relative to prediction year (rather than data year, as in args)
oat_lag3 <- oat_wrap(iter = 3000, chain = 4, oat_data_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 2, lag_hvst = 2, lag_MS = 1, lag_smolt = 0)
oat_lag2 <- oat_wrap(iter = 3000, chain = 4, oat_data_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 1, lag_hvst = 1, lag_MS = 1, lag_smolt = 0)
oat_lag1 <- oat_wrap(iter = 3000, chain = 4, oat_data_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 0, lag_hvst = 0, lag_MS = 1, lag_smolt = 0)

```

# Results

Saved output from the one-ahead runs is read back into memory and joined with observations to allow examination of performance skill.

```{r read_oat_and_oat_obs}
# #tibbles of annual predictions for AR1 & STIPM with a given lag
# bind_rows(
#   list.files("oat_fits", pattern = "lags_spwn0_hvst0", full.names = T) |>
#     map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l1_", mod))
#   ,
#   list.files("oat_fits", pattern = "lags_spwn1_hvst1", full.names = T) |>
#     map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l2_", mod))
#   ,
#   list.files("oat_fits", pattern = "lags_spwn2_hvst2", full.names = T) |>
#     map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l3_", mod))
# ) |>
#   select(lag_mod, year, StockID, pop_id, pop, n_diverg, n_eff, Rhat, `2.5%`:`97.5%`) |>
#   saveRDS("oat_fits/oat_l0_l1_l2_2009_2019.rds")

oat <- readRDS("oat_fits/oat_l0_l1_l2_2009_2019.rds")

(
oat_obs <- left_join(
  oat,
  coho_data_tbl |> select(year, StockID, rtrn) |> 
    #adding MASE denominator
    group_by(StockID) |> 
    mutate(
      scale_err = mean(abs(rtrn - c(NA, head(rtrn, -1))), na.rm = T)
    ) |> 
    ungroup(),
  by = c("year", "StockID")
  ) |>
  select(lag_mod, year, StockID, pop_id, pop, scale_err, rtrn, `2.5%`:`97.5%`) |> 
  mutate(
    in_50 = (rtrn >= `25%`) & (rtrn <= `75%`),
    in_95 = (rtrn >= `2.5%`) & (rtrn <= `97.5%`),
    #convention: positive as overforecast ("fewer returned than predicted"), negative underforecast 
    err = `50%` - rtrn, 
    err_abs = abs(err),
    err_log = log(`50%`) - log(rtrn),
    err_pct = err / rtrn,
    err_abs_pct = err_abs / rtrn,
    lar = log(`50%`/rtrn), #log accuracy ratio, used for median symmetric accuracy as described in Morley et al 2018
    #kept as Eq26 in paper with pred-obs
    mase = abs(`50%` - rtrn) / scale_err
  )
)

#oat |> count(lag_mod) ; oat_obs |> count(lag_mod)
```

We can also generate deterministic trailing means for comparison.

```{r trailing_means}
#create a temp col of the best case values that would be available in a given forecast year (i.e., up to year - 1)
#then create rolling mean over prior values
rtrn_trail_mean <- coho_data_tbl |> 
  select(StockID, pop_id, pop, year, rtrn) |>
  group_by(StockID, pop_id, pop) |> 
  mutate(
    lag3_rtrn = c(NA, NA, NA, head(rtrn, -3)),
    l3_trail_mean = slider::slide_dbl(lag3_rtrn, ~mean(., na.rm=T), .before = 2),
    lag2_rtrn = c(NA, NA, head(rtrn, -2)),
    l2_trail_mean = slider::slide_dbl(lag2_rtrn, ~mean(., na.rm=T), .before = 2)
  ) |> 
  ungroup() |> 
  filter(between(year, 2009, 2019)) |> select(-starts_with("lag")) |>  
  pivot_longer(cols = contains("trail"), names_to = "lag_mod", values_to = "50%") |> 
  mutate(
    err = `50%` - rtrn, 
    err_abs = abs(err),
    err_log = log(`50%`) - log(rtrn),
    err_pct = err / rtrn,
    err_abs_pct = err_abs / rtrn,
    lar = log(`50%`/rtrn)
  )

```

## Diagnostics

```{r oat_rhat_neff_n_diverg}
# #all years, all pops
# oat |> 
#   group_by(lag_mod) |> 
#   summarise(
#     n_diverg = max(n_diverg),
#     Rhat_med = median(Rhat), Rhat_max = max(Rhat),
#     n_eff_med = median(n_eff), n_eff_min = min(n_eff),
#     .groups = "drop")

# #by year, all pops
# oat |> 
#   group_by(lag_mod, year) |> 
#   summarise(
#     n_diverg = max(n_diverg),
#     Rhat_med = median(Rhat), Rhat_max = max(Rhat),
#     n_eff_med = median(n_eff), n_eff_min = min(n_eff),
#     .groups = "drop") |> 
#   #  print(n = 100)
#   gt::gt(rowname_col = "lag_mod") |>
#   gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "Willapa Bay Natural") |>
#   gt::fmt_number(contains("Rhat"), decimals = 2) |>
#   gt::fmt_number(contains("n_eff"), decimals = 0)

#Willapa, all years, similar to "all pops"
oat |> 
  filter(StockID == 161) |> 
  # arrange(lag_mod, year) |>
  # #filter(str_detect(lag_mod, "stipm")) |> 
  # print(n = 100)
  group_by(lag_mod) |> 
  summarise(
    n_diverg = max(n_diverg),
    Rhat_med = median(Rhat), Rhat_max = max(Rhat),
    n_eff_med = median(n_eff), n_eff_min = min(n_eff),
    .groups = "drop") |> 
  gt::gt(rowname_col = "lag_mod") |> 
#  gt::cols_hide(c("pop_id", "pop")) |> 
  gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "Willapa Bay Natural") |> 
  gt::fmt_number(contains("Rhat"), decimals = 2) |> 
  gt::fmt_number(contains("n_eff"), decimals = 0)

# #n_eff boxes
# oat |> 
#   filter(StockID == 161, str_detect(lag_mod, "stipm")) |> 
#   ggplot(aes(lag_mod, n_eff)) + geom_boxplot() + geom_jitter(width = 0.1)
# #n_eff col timeseries, arguably slightly better after 2012
# oat |> 
#   filter(StockID == 161, str_detect(lag_mod, "stipm")) |> 
#   ggplot(aes(factor(year), n_eff, fill = lag_mod)) + geom_col(position = "dodge") + scale_fill_grey()

```

## One-ahead Performance

```{r}
pal_lag_mod <- c("purple", "gold", "cyan") #ar1, stipm, trailing arith mean
```

### Lag3 paneled by year

```{r oat_obs_pred_patchwork}
set_names(2008:2018) |> 
  map(function(x) {
    d <- coho_data_tbl |> filter(StockID == 161, between(year, 1998, x)) |> select(year, StockID, pop, rtrn) |> mutate(year = as.character(year))
    # d <- stan_data_filter(coho_data_tbl, data_year_min = 1998, data_year_max = x, lag_spwn = 2, lag_hvst = 2, lag_MS = 1, lag_smolt = 0) |> 
    #    pluck("rtrn") |> filter(StockID == 161) |> select(year, StockID, pop, rtrn) |> mutate(year = as.character(year))
    
    oat_obs |> 
      filter(StockID == 161, str_detect(lag_mod, "l3"), year == x+1) |> mutate(year = as.character(year)) |> 
      ggplot() +
      #all pre-forecast data
      geom_col(data = d, aes(x = year, y = rtrn), fill = grey(0.8), width = 0.5) +
      #available data for L3
      geom_col(data = d |> filter(year %in% as.character(1998:x-2)), aes(x = year, y = rtrn), fill = grey(0.4), width = 0.5) +
      #observed in forecast year
      geom_point(aes(x = year, y = rtrn), color = 1, shape = 17)  +
      #forecasts
      geom_pointrange(aes(x = year, y = `50%`, ymin = `25%`, ymax = `75%`, color = lag_mod), fatten = 1.1, position = position_dodge(width = 0.5), show.legend = F) +
      geom_point(aes(x = year, y = `50%`, color = lag_mod), position = position_dodge(width = 0.5), show.legend = F) +
      geom_vline(xintercept = as.character(x-2), linetype = "dotted", size = 1) +
      scale_x_discrete(name = "", limits = as.character(1998:2020), drop = FALSE) +
      scale_y_continuous(name = "Return", limits = c(0,125000), labels = scales::comma) + #limits drops point error when outside ymax
      scale_color_manual(name = "", values = pal_lag_mod) +
      theme(axis.text = element_text(size = 6), axis.title = element_text(size = 5))
    #+ labs(subtitle = paste("Predicting", x+1))
}) |> 
  patchwork::wrap_plots(ncol = 1)

ggsave("O:/code/coho/forecast_wb/f_oat_l3_by_pred_year.png", width = 7, height = 9)
```

### Lag3 50th with ribbons

```{r l3_50th_ribbons}
oat_ribbon <- function(stk = 161, lg_md = "l3"){
  d <- oat_obs |> filter(StockID == stk, str_detect(lag_mod, lg_md))
  ggplot(d, aes(year)) + 
    geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 2009, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    #geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`, color = lag_mod, fill = lag_mod), alpha = 0.2) + 
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`, color = lag_mod, fill = lag_mod), alpha = 0.4) + 
    geom_line(aes(y = `50%`, color = lag_mod), size = 1.2) +
    scale_x_continuous(name = "", breaks = 1998:2020, labels = 1998:2020, guide = guide_axis(n.dodge = 2)) +
    scale_y_continuous(name = "Return", labels = scales::comma) +
    scale_color_manual(name = "", values = c("purple", "gold"), na.translate = FALSE, aesthetics = c("color", "fill")) +
    theme(legend.position = "top") +
    labs(title = d$pop[1])
}

oat_ribbon()
ggsave("O:/code/coho/forecast_wb/f_oat_l3_pred_single_panel.png", width = 7, height = 5)

# oat_ribbon(131) #Quill Fall, accurate
# oat_ribbon(153) #Hump, STIPM very good in later years
# oat_ribbon(111) #Elwha, wacky
# oat_ribbon(157) #GH, bouncy
# oat_ribbon(35) #Snohomish, STIPM def looks better but MSA worst...

```

### Rank order of WB returns 1998:2019

```{r wb_rtrn_rank_order}
coho_data_tbl |> 
  filter(StockID == 161, between(year, 1998, 2019)) |> 
  mutate(year = factor(year)) |> 
  ggplot(aes(fct_reorder(year, rtrn, min, .desc = T), rtrn)) +
  geom_col() +
  scale_x_discrete("Return year ordered by magnitude", guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous(name = "Return", labels = scales::comma)

ggsave("O:/code/coho/forecast_wb/f_wb_return_rank_ordered.png", width = 6, height = 4)
```

### Lag3 error time series

```{r l3_error_series}
ggdata <- bind_rows(oat_obs, rtrn_trail_mean) |> 
  filter(StockID == 161, str_detect(lag_mod, "l3")) |> 
  arrange(year, lag_mod) |> 
  mutate(year = as.character(year))

ggplot(ggdata, aes(year)) + 
  geom_col(aes(y = err, color = lag_mod, fill = lag_mod), position = position_dodge(), width = 0.7) +
  geom_hline(
    data = ggdata |> group_by(lag_mod) |> summarise(err = mean(err), .groups = "drop"),
    aes(yintercept = err, color = lag_mod),
    linetype = "dashed", size = 1.1) +
  scale_x_discrete(name = "", guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous(name = "Error, 50th - obs", labels = scales::comma) + #limits drops point error when outside ymax
  scale_color_manual(name = "", values = pal_lag_mod, na.translate = FALSE, aesthetics = c("color", "fill")) +
  theme(legend.position = "top")

ggsave("O:/code/coho/forecast_wb/f_oat_l3_error.png", width = 7, height = 5)


# ggdata <- bind_rows(oat_obs, rtrn_trail_mean) |> 
#   filter(StockID == 161, str_detect(lag_mod, "l3|l2")) |> 
#   arrange(year, lag_mod) |> 
#   mutate(
#     year = as.character(year),
#     lag = str_sub(lag_mod, 1,2),
#     mod = str_sub(lag_mod, 4, 40))
# 
# ggplot(ggdata, aes(year)) + 
#   geom_col(aes(y = err, color = mod, fill = mod), position = position_dodge(), width = 0.7) +
#   geom_hline(
#     data = ggdata |> group_by(lag_mod, lag, mod) |> summarise(err = mean(err), .groups = "drop"),
#     aes(yintercept = err, color = mod),
#     linetype = "dashed", size = 1.1) +
#   scale_x_discrete(name = "", guide = guide_axis(n.dodge = 2)) +
#   scale_y_continuous(name = "Error, 50th - obs", labels = scales::comma) + #limits drops point error when outside ymax
#   scale_color_manual(name = "", values = pal_lag_mod, na.translate = FALSE, aesthetics = c("color", "fill")) +
#   facet_wrap(~lag, scales = "free") +
#   theme(legend.position = "top")

```

### Summarized performance measures

```{r gt_perf_measures}

bind_rows(oat_obs, rtrn_trail_mean) |>
  arrange(year, lag_mod) |>
  group_by(StockID, pop, lag_mod) |> 
  summarise(
    msa = 100*(exp(median(abs(lar))) - 1),
    me = mean(err),
    mpe = 100*median(err_pct),
    rmse = sqrt(mean(err^2)),
    .groups = "drop") |> 
  mutate(
    lag = paste("Lag", str_sub(lag_mod, 2,2)),
    mod = toupper(str_sub(lag_mod, 4, 40))
    ) |> 
  filter(StockID == 161) |>
  gt::gt(rowname_col = "mod", groupname_col = "lag") |> 
  gt::cols_hide(c(StockID, pop, lag_mod)) |> 
  gt::fmt_percent(columns = c(msa, mpe), scale_values = F, decimals = 1) |> 
  gt::fmt_number(columns = c(me, rmse), decimals = 0) |> 
  gt::data_color(
    columns = msa, 
    colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,200), na.color = "red")
  ) |> 
  gt::tab_header(
    title = "One-ahead Performance Measures", 
    subtitle = "AR1 and STIPM calculated from posterior median") |> 
  gt::tab_source_note("Median Symmetric Accuracy (MSA), Mean Error (ME)") |> 
  gt::tab_source_note("Median Percent Error (MPE), Root Mean Square Error (RMSE)") |> 

  gt::gtsave("O:/code/coho/forecast_wb/gt_oat_perf_measures.png", expand = 20)

# oat_obs_msa <- bind_rows(oat_obs, rtrn_trail_mean) |>
#   arrange(year, lag_mod) |>
#   group_by(StockID, pop, lag_mod) |> 
#   summarise(
#     msa = 100*(exp(median(abs(lar))) - 1)
#     , .groups = "drop")

# #some series, e.g., Quillayute Fall (131), are much more accurately forecast...
# #looks like some are just screwy
# oat_obs_msa |> 
#   filter(str_detect(lag_mod, "l3|l2")) |> 
#   pivot_wider(names_from = lag_mod, values_from = msa) |> 
#   arrange(l3_stipm) |> #print(n=40) 
#   gt::gt() |> 
#   gt::fmt_number(columns = -c(StockID, pop), decimals = 1) |> 
#   gt::grand_summary_rows(columns = -c(StockID, pop), fns = list(median = "median", mean = "mean")) |> 
#   gt::data_color(
#     columns = -c(StockID, pop), 
#     colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,400), na.color = "red")
#   ) |> 
#   gt::tab_header("One-ahea Median Symmetric Accuracy (MSA)", subtitle = "Posterior median for AR1 and STIPM predictions")

# #older summaries across other measures
# oat_obs |> 
#   group_by(StockID, pop_id, pop, lag_mod) |> 
#   summarise(across(err:mase, list(mean = ~mean(.), median = ~median(.))), .groups = "drop") |> 
#   select(StockID:lag_mod, contains("median"), contains("mean")) |> 
#   filter(StockID == 161)
  
```

### Overlaid posterior medians by lag

```{r oat_lag_overlay}
oat_lag_overlay <- function(stk = 161){
  d <- oat_obs |> filter(StockID == stk) |> 
    mutate(
      lag = paste("Lag", str_sub(lag_mod, 2,2)),
      mod = toupper(str_sub(lag_mod, 4, 40))
    )
 
  ggplot(d, aes(year)) + 
    geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 2009, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    geom_line(aes(y = `50%`, color = lag), size = 1.2) +
    scale_x_continuous(name = "", breaks = 1998:2020, labels = 1998:2020, guide = guide_axis(n.dodge = 2)) +
    scale_y_continuous(name = "Return", labels = scales::comma) + 
    scale_color_manual("", values = as.vector(wacolors::wa_pal("rainier", which = c("ground","winter_sky","paintbrush"))), aesthetics = c("color", "fill")) +
    facet_wrap(~mod, ncol = 1) + 
    theme(legend.position = "top") +
    labs(title = d$pop[1], color = "", fill = "")
  
}

oat_lag_overlay()

ggsave("O:/code/coho/forecast_wb/f_oat_pred_lag_overlay.png", width = 7, height = 9)
```



```{r wb_plots}
#error: pred - obs, 
oat_obs |> filter(StockID == 161) |> 
  ggplot(aes(year, err)) + geom_col() + facet_wrap(~lag_mod, ncol = 2)

#percent error
oat_obs |> filter(StockID == 161) |> 
  ggplot(aes(year, err_pct)) + geom_col() + facet_wrap(~lag_mod, ncol = 2)

pre_oat <- coho_data_tbl |> filter(between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn)
coho_data_tbl |> filter(StockID == 161, between(year, 2009, 2010)) |> summarise(rtrn = mean(rtrn)) #108082
coho_data_tbl |> filter(StockID == 161, between(year, 2018, 2019)) |> summarise(rtrn = mean(rtrn)) #18212
#pred lines on obs cols
oat_obs |> filter(StockID == 161) |> 
  ggplot(aes(year)) + 
  geom_col(data = filter(pre_oat, StockID == 161), aes(y = rtrn), fill = grey(0.8)) +
  geom_col(aes(y = rtrn)) +
  geom_line(aes(y = `50%`), color = 4, size = 1.2) + 
  annotate("text", x = 2011, y = 110000, label = "avg 108K\n2009-10", hjust = 0) +
  annotate("text", x = 2017, y = 70000, label = "avg 18K\n2018-19", hjust = 0) +
  facet_wrap(~lag_mod, ncol = 2)

#same thing, lags as color
#pred lines on obs cols
oat_obs |> filter(StockID == 161) |> 
  separate(lag_mod, c("lag", "mod")) |> 
  ggplot(aes(year)) + 
  geom_col(data = coho_data_tbl |> filter(StockID == 161, between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.8)) +
  geom_col(data = coho_data_tbl |> filter(StockID == 161, between(year, 2009, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn)) +
  geom_line(aes(y = `50%`, color = lag), size = 1.2) + 
  annotate("text", x = 2011, y = 110000, label = "avg 108K\n2009-10", hjust = 0) +
  annotate("text", x = 2017, y = 70000, label = "avg 18K\n2018-19", hjust = 0) +
  facet_wrap(~mod, ncol = 1)


#scatter showing 50+IQR
oat_obs |> filter(StockID == 161) |> 
  ggplot(aes(rtrn, `50%`, label = year)) + 
  geom_abline(slope = 1, intercept = 0) +
  geom_point() + 
  geom_pointrange(aes(ymin = `25%`, ymax = `75%`)) +
  geom_text(hjust = -0.2) + 
  scale_x_continuous(limits = c(0, 150000), labels = scales::comma) +
  scale_y_continuous("One-ahead forecast", limits = c(0, 150000), labels = scales::comma) +
  facet_wrap(~lag_mod, ncol = 2)

#scatter illustrating relative over/underprediction at different posterior percentiles
oat_obs |> filter(StockID == 161) |> 
  select(lag_mod, year, StockID, pop, rtrn:`75%`) |> 
  #filter(str_detect(lag_mod, "l1_stipm|l2_stipm")) |> 
  filter(str_detect(lag_mod, "tipm")) |> 
  pivot_longer(cols = `2.5%`:`75%`, names_to = "pctnl", values_to = "pred") |> 
  ggplot(aes(rtrn, pred, label = year)) + 
  geom_abline(slope = 1, intercept = 0) +
  geom_point() + geom_text() + 
  scale_x_continuous(limits = c(0, 150000), labels = scales::comma) +
  scale_y_continuous(limits = c(0, 150000), labels = scales::comma) +
  facet_wrap(~lag_mod+pctnl, ncol = 4)

#focus on highly conservative scatter on worst case data lag
oat_obs |> filter(StockID == 161) |> 
  filter(str_detect(lag_mod, "l2_stipm")) |> 
  ggplot(aes(rtrn, `2.5%`, label = year)) + 
  geom_abline(slope = 1, intercept = 0) +
  geom_point() + geom_text() + 
  scale_x_continuous(limits = c(0, 120000), labels = scales::comma) +
  scale_y_continuous(limits = c(0, 75000), labels = scales::comma) +
  facet_wrap(~lag_mod, ncol = 2)

```

```{r oat_obs_mase_by_lag, eval=FALSE}
#MASE ratios by lag
oat_obs |> 
  select(pop_id, pop, lag_mod, year, mase) |>
  pivot_wider(names_from = lag_mod, values_from = mase) |> 
  mutate(
    l1_l2_ar1 = l1_ar1 / l2_ar1,
    l1_l2_stipm = l1_stipm / l2_stipm,
    l1_stipm_ar1 = l1_stipm / l1_ar1
  ) |> 
  # group_by(pop_id, pop) |> 
  # summarise(across(starts_with("l1_l2"), list(min = ~min(.), med = ~median(.), max = ~max(.)))) |> print(n = 50)
  filter(pop_id == 34) |>
  select(pop, year, l1_ar1, l2_ar1, l1_l2_ar1, l1_stipm, l2_stipm, l1_l2_stipm, l1_stipm_ar1) |> 
  gt() |> 
  fmt_number(l1_ar1:l1_stipm_ar1, decimals = 2) |> 
  tab_spanner("AR1", l1_ar1:l1_l2_ar1) |> 
  tab_spanner("STIPM", contains("stipm"))


```

```{r oat_obs_l1_vs_l2_stipm, eval=FALSE}
# #WB focus
# coho_data_tbl |> filter(pop_id == 34) |> select(pop_id:StockID, year, spwn, hvst, rtrn) |> print(n = 50)
# oat_obs |> filter(pop_id == 34) |> filter(str_detect(lag_mod, "stipm"))

#compare lags for STIPM
oat_obs |> 
  select(pop_id, lag_mod, year, rtrn:`90%`, err:mase, in_10_90) |>
  filter(str_detect(lag_mod, "stipm")) |> arrange(lag_mod, year) |> 
  filter(pop_id == 34) |>
  #gt(groupname_col = "year", rowname_col = "lag_mod")
  gt(groupname_col = "lag_mod", rowname_col = "year") |> 
  cols_hide(pop_id) |> 
  fmt_number(rtrn:err_abs, decimals = 0) |> 
  fmt_number(err_log:mase, decimals = 2) |> 
  summary_rows(groups = TRUE, fns = list(min = ~min(.), med = ~median(.), max = ~max(.)))
```

```{r oat_obs_ar1_vs_stipm_l1, eval=FALSE}
#compare STIPM vs AR1 lag1    
oat_obs |> 
  select(pop_id, lag_mod, year, rtrn:`90%`, err:mase, in_10_90) |>
  filter(str_detect(lag_mod, "l1")) |> arrange(lag_mod, year) |> 
  filter(pop_id == 34) |>
  #gt(groupname_col = "year", rowname_col = "lag_mod")
  gt(groupname_col = "lag_mod", rowname_col = "year") |> 
  cols_hide(pop_id) |> 
  fmt_number(rtrn:err_abs, decimals = 0) |> 
  fmt_number(err_log:mase, decimals = 2) |> 
  summary_rows(groups = TRUE, fns = list(min = ~min(.), med = ~median(.), max = ~max(.)))
```

```{r odds_and_ends, eval=FALSE}
# rr <- bind_cols(
#   readxl::read_excel("O:/code/coho/forecast_wb/2021 WB4 Coho Forecast Model DRAFT 12.14.2020.xlsx", range = "RR!C38:C48", col_names = "year"),
#   readxl::read_excel("O:/code/coho/forecast_wb/2021 WB4 Coho Forecast Model DRAFT 12.14.2020.xlsx", range = "RR!E38:E48", col_names = "escp"),
#   readxl::read_excel("O:/code/coho/forecast_wb/2021 WB4 Coho Forecast Model DRAFT 12.14.2020.xlsx", range = "RR!U38:U48", col_names = "catch")) |> 
#   mutate(trs = escp + catch)


#all years, all pops
oat_obs |> 
  group_by(lag_mod) |> 
  summarise(
    n = n(),
    in_10_90_sum = sum(in_10_90),
    in_10_90_pct = in_10_90_sum / n,
    across(err:mase, median),
    .groups = "drop")

#all years by pop
oat_obs |> 
  group_by(lag_mod, pop_id, pop) |> 
  summarise(
    n = n(),
    in_10_90_sum = sum(in_10_90),
    in_10_90_pct = in_10_90_sum / n,
    across(err:mase, median),
    .groups = "drop") |> 
  filter(pop_id == 34)

# #values are terminal run sizes from regional workbook
# #catches do not include preterm, so escapements line up but t9$obs != stipm$rtrn 
# t9 <- readxl::read_excel("O:/code/coho/forecast_wb/table9_2020_submission.xlsx") |> 
#   mutate(
#     err = obs - mar_surv,
#     err_abs = abs(err),
#     err_log = log(obs) - log(mar_surv),
#     err_abs_pct = err_abs / obs
#   )
# 
# t9 |> filter(between(year, 2013, 2018)) |> summarise(across(err:err_abs_pct, median))
# oat_obs |> arrange(mod, year) |> 
#   filter(pop_id == 34) |> select(mod, year, rtrn, `10%`:`90%`, err:err_abs_pct) |> group_by(mod) |> summarise(across(err:err_abs_pct, median))

```


```{r oat_obs_pred_gganimate, eval=FALSE}
animate(
  set_names(2009:2017) |> 
    map_df(function(x) {
      full_join(
        coho_data_tbl |> 
          filter(pop_id == 34, between(year, 1998, x)) |> 
          select(year, pop_id, train = rtrn) |> 
          mutate(year = as.character(year))
        ,
        oat_obs |> 
          filter(pop_id == 34, year == x+1, str_detect(lag_mod, "l1")) |> 
          select(year, pop_id, pop, lag_mod, rtrn:`90%`) |> 
          mutate(year = as.character(year))
        , 
        by = c("pop_id", "year")
      ) |> 
        mutate(year_pred = x+1, year_data = as.character(x-1))
    }) |> 
    ggplot(aes(x = year)) +
    geom_pointrange(aes(y = `50%`, ymin = `10%`, ymax = `90%`, color = lag_mod), position = position_dodge2(width = 0.5), size = 1.2) +
    geom_point(aes(y = `50%`, color = lag_mod), position = position_dodge2(width = 0.5), size = 1.5) +
    geom_point(aes(y = rtrn), color = "tan", shape = 15, size = 4)  +
    #  geom_point(aes(y = train), color = "brown", shape = 16) +
    geom_col(aes(y = train), color = "tan", fill = "tan", width = 0.3, alpha = 0.7) +
   # geom_vline(aes(xintercept = year_data), linetype = "dashed") +
    scale_x_discrete(name = "", limits = as.character(1998:2018), drop = FALSE, guide = guide_axis(n.dodge = 2)) +
    scale_y_continuous(name = "Return", limits = c(0, 130000), labels = scales::comma) + #limits drops point error when outside ymax
    scale_color_manual(name = "", values = c("grey", "gold"), na.translate = FALSE) + 
    theme(legend.position = "top") +
    transition_states(year_pred, 3, 1) #+ shadow_mark(alpha = 0.4)
  ,
  width = 7, height = 5, units = "in", res = 100
  )

anim_save("oat_l1_anim.gif")

```

# REVISE Full dataset

```{r full_data, eval=FALSE}
iter <- 400
chain <- 2
data_year_min <- 1998
data_year_max <- 2019

(stan_data_list <- stan_data_filter(coho_data_tbl, 
                                   data_year_min = data_year_min, 
                                   data_year_max = data_year_max,
                                   lag_spwn = 0, lag_hvst = 0))
#str(stan_data_list, max.level = 1)
print(stan_data_list$y_min_max)
print(c("smolt", range(stan_data_list$smolt$year)))
print(c("MS", range(stan_data_list$MS$year)))
print(c("spwn", range(stan_data_list$spwn$year)))
print(c("hvst", range(stan_data_list$hvst$year)))
print(c("rtrn", range(stan_data_list$rtrn$year)))

fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = iter, n_chain = chain)

fit_ar1_pred <- summary(fit_ar1, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #should be x+1
    mod = "ar1",
    n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |> 
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = iter, n_chain = chain)

fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #should be x+1
    mod = "stipm",
    n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |> 
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

```


# Can delete

```{r test_2050s, eval=FALSE}
#further into future just with larger n_year? oh yes.

fit_ar1_test <- stan(
  file = 'LD_coho_forecast_AR_ind_2.stan',
  iter = 200, chains = 2, thin = 1, seed = 222,
  control = list(adapt_delta = 0.99, max_treedepth = 10.25),
  data = list(
    #Number of years (total, includes several missing years for some stocks)
    n_year = length(unique(tbl_coho$year))*2,
    #Number of total populations    
    n_pop = length(unique(tbl_coho$pop_id)),
    #Number of populations with return data
    n_pop_tot = length(unique(tbl_coho_stan$rtrn$pop_id)),
    #Which populations possess return data
    pop_tot = unique(tbl_coho_stan$rtrn$pop_id),
    #Length of the return data vectors
    n_tot = nrow(tbl_coho_stan$rtrn), #length(tot_dat)
    #Vectors of all return data across all populations
    tot_dat = tbl_coho_stan$rtrn$rtrn,
    #Vectors of the indices identifying which years are those with non-NA data for the return data
    tot_true = tbl_coho_stan$rtrn$yr,
    #Paired vectors of slice points indicating the beginning, and end of the data for a particular population
    slice_tot_start = tbl_coho_stan$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
    slice_tot_end = tbl_coho_stan$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )

fit_ar1_test_smry <- fit_smry(fit_ar1_test)

fit_ar1_test_smry$adult_est |> #tail() 
  filter(pop_id > 30) |> 
  ggplot(aes(x = year)) +
  scale_y_continuous("Estimated adult return", labels = scales::comma) +
  scale_fill_brewer(type = "qual", aesthetics = c("fill", "color") ) +
  geom_ribbon(aes(ymin = `10%`, ymax = `90%`), alpha = 0.2) + 
  geom_line(aes(y = `50%`)) +
  geom_point(
    data = tbl_coho_stan$rtrn |> filter(pop_id > 30) |> select(year, pop, pop_id, rtrn),
    aes(y = rtrn),
    inherit.aes = T) +
  facet_wrap(~pop)

```

