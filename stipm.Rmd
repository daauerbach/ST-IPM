---
title: "stipm"
author: "dan.auerbach@dfw.wa.gov modifying DeFilippo et al. 2021 in consultation with Thomas.Buehrens@dfw.wa.gov"
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
output: 
  wdfwTemplates::wdfw_html_format

---

This script updates and extends the [original implementation](https://github.com/lukasdefilippo/ST-IPM) of [DeFilippo et al.'s (2021)](https://www.sciencedirect.com/science/article/pii/S0165783621001429) "Spatiotemporal Integrated Population Model" (ST-IPM) of Washington state natural origin coho (*O. kisutch*) returns, with initial application to annual preseason forecasts of Willapa Bay returning adults.

# setup

```{r setup, results = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, results = FALSE, warning = FALSE, message = FALSE)

library("tidyverse")
library("gt")
library("odbc"); library("DBI")
library("rstan")
options(mc.cores = 10) #12 on DA machine
rstan_options(auto_write = TRUE)
theme_set(theme_light())


fp <- list(
  # coho_file = 'Coho data_3-30-20.csv',
  # cwt_file = 'CWT_FRAM_Matches_complete20200303.csv',
  # stream_file = 'Coho_KM_3.31.2020_2.csv',
  data_fram = "data/postseason_fram_spwn_hvst_ests.csv",
  data_cwt = "data/rmis_cwts.csv",
  data_smolt = "data/smolt_outmigrants.csv",
  data_full = "data/fram_cwt_smolt_fulljoin.csv"
  )

# ## dropped FRAM StockID 13 and 43, original pop_ids 10 and 24
# # fram_stocks <- c(105,93,89,45,51,55,75,81,61,23,149,63,107,115,111,157,97,135,153,101,69,1,85,139,131,127,145,11,17,59,35,29,117,161)
# 
# pop_meta <- readxl::read_excel("pop_meta.xlsx") |>
#   dplyr::select(pop_id:hat) 
# #deleted and reindexed pop in file, no longer need: |> dplyr::filter(StockID != 13, StockID != 43)

coho_data_tbl <- readr::read_csv(fp$data_full)

sf_coord <- coho_data_tbl |> 
  distinct(StockID, StockLongName, pop_id, pop, lon, lat, hab_km) |> 
  sf::st_as_sf(coords = c("lon", 'lat'), crs = sf::st_crs("+proj=longlat +datum=WGS84")) |> 
  sf::st_transform(crs = sf::st_crs("+proj=utm +zone=10T ellps=WGS84"))

# ggplot(sf_coord) + geom_sf(aes(color = hab_km, size = hab_km)) + geom_sf_text(aes(label = pop_id))


##load("stipm.Rdata")

```

# data

The analysis relies on per-stock escapement and harvest estimates compiled in post-season [coho FRAM](https://github.com/FRAMverse/fram_doc) runs as well as CWT-based marine survival estimates derived from RMIS, and smolt trap outmigrant estimates compiled by WDFW.

## FRAM postseason estimates of spawning escapement and harvest mortality

Coho FRAM tracks age 3 fish across 5 time steps corresponding to a calendar year. It includes unmarked and marked units of both natural and hatchery stocks. 

Data in the best available database extend to 1986, but all values prior to 1998 and some prior to 2004 are of unknown origin.

```{r fram_readers, eval=FALSE}
read_coho_backwards <- function (db, runs = NULL, stocks = NULL) {

  db_con <- DBI::dbConnect(drv = odbc::odbc(),
    .connection_string = paste0("Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=", db, ";"))

  #lazy, full table then reduce as specified
  bk <- dplyr::tbl(db_con, "BackwardsFRAM") |> dplyr::select(RunID, StockID, escp = TargetEscAge3)
  if (!is.null(runs)) { bk <- dplyr::filter(bk, RunID %in% runs) }
  if (!is.null(stocks)) { bk <- dplyr::filter(bk, StockID %in% stocks) }

  #associate metainfo and pull
  bk <- bk |> 
    dplyr::left_join(
      dplyr::tbl(db_con, "RunID") |>  dplyr::select(RunID, RunYear, RunName), 
      by = "RunID") |>
    dplyr::left_join(
      dplyr::tbl(db_con, "Stock") |> dplyr::filter(Species == "COHO") |> dplyr::select(StockID, StockLongName),
      by = "StockID") |>
    dplyr::collect() |> 
    dplyr::arrange(RunYear, StockID)

  DBI::dbDisconnect(db_con)
  
  return(bk)
}

read_coho_escapement <- function (db, runs = NULL, stocks = NULL) {

  db_con <- DBI::dbConnect(drv = odbc::odbc(),
    .connection_string = paste0("Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=", db, ";"))
  #coho is already only age 3 in TimeStep 5
  #lazy, full table then reduce as specified
  escp <- dplyr::tbl(db_con, "Escapement") |> dplyr::select(RunID, StockID, escp = Escapement)
  if (!is.null(runs)) { escp <- dplyr::filter(escp, RunID %in% runs) }
  if (!is.null(stocks)) { escp <- dplyr::filter(escp, StockID %in% stocks) }

  #associate metainfo and pull
  escp <- escp |> 
    dplyr::left_join(
      dplyr::tbl(db_con, "RunID") |>  dplyr::select(RunID, RunYear, RunName), 
      by = "RunID") |>
    dplyr::left_join(
      dplyr::tbl(db_con, "Stock") |> dplyr::filter(Species == "COHO") |> dplyr::select(StockID, StockLongName),
      by = "StockID") |>
    dplyr::collect() |> 
    dplyr::arrange(RunYear, StockID)

  DBI::dbDisconnect(db_con)
  
  return(escp)
}

read_coho_mort <- function (db, runs = NULL, stocks = NULL) {
  
  db_con <- DBI::dbConnect(drv = odbc::odbc(),
    .connection_string = paste0("Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=", db, ";"))
  
  #lazy, full table then reduce as specified
  m <- dplyr::tbl(db_con, "Mortality") |> dplyr::select(-PrimaryKey)
  if (!is.null(runs)) { m <- dplyr::filter(m, RunID %in% runs) }
  if (!is.null(stocks)) { m <- dplyr::filter(m, StockID %in% stocks) }

  #associate metainfo and pull
  m <- m |> 
    dplyr::left_join(
      dplyr::tbl(db_con, "RunID") |>  dplyr::select(RunID, RunYear, RunName), 
      by = "RunID") |>
    dplyr::left_join(
      dplyr::tbl(db_con, "Stock") |> dplyr::filter(Species == "COHO") |> dplyr::select(StockID, StockLongName),
      by = "StockID") |>
    dplyr::left_join(
      dplyr::tbl(db_con, "Fishery") |> dplyr::filter(Species == "COHO") |> dplyr::select(FisheryID, FisheryName), 
      by = "FisheryID") |> 
    dplyr::collect() |> 
    dplyr::arrange(RunYear, FisheryID, TimeStep, StockID)

  DBI::dbDisconnect(db_con)
  
  return(m)
}
```

```{r read_and_export_fram_tables, eval=FALSE}
#the relevant data are queried and re-exported as csv
#stays similar to original and allows greater reproducibility/portability
mdb <- "O:/code/coho_fram_validation/PSC_CoTC_PostSeason_CohoFRAMDB_thru2019_021021.mdb"
#appears to be a mysterious RunID 3 in the Escapement and Mortality tables but not RunID table...
#aggregate mortality across timesteps for all sources to "estimated total harvest related impacts"
fram <- full_join(
  read_coho_escapement(mdb, stocks = pop_meta$FRAM_StockID) |> 
    filter(!is.na(RunYear))
  ,
  read_coho_mort(mdb, stocks = pop_meta$FRAM_StockID) |>
    filter(!is.na(RunYear)) |> 
    mutate(mort = LandedCatch + NonRetention + Shaker + DropOff + MSFLandedCatch + MSFNonRetention + MSFShaker + MSFDropOff) |> 
    group_by(RunYear, StockID, StockLongName) |> 
    summarise(mort = sum(mort), .groups = "drop")
  ,
  by = c("RunYear", "StockID", "StockLongName")
  ) |> 
  select(year = RunYear, StockID, StockLongName, spwn = escp, hvst = mort)

#fram |> filter(StockID == 161) |> print(n = 100)

write_csv(fram, "data/postseason_fram_spwn_hvst_ests.csv")

```

## Smolt outmigrant estimates

Dataset updated by Marisa Litz.

```{r request_to_ML, eval=FALSE}

#tbl_coho |> filter(!is.na(`Smolt Abundance`), year >= 1986) |> count(pop)

smolt_used <- stan_data_filter(coho_data_tbl, year_min = 1986, year_max = 2021, lag_spwn = 1, lag_hvst = 1) |> 
  getElement("smolt") 
#writexl::write_xlsx(smolt_used, "O:/code/coho/forecast_wb/smolt_outmig_to_update.xlsx")

smolt_used |> group_by(pop) |> summarise(ymin = min(year), ymax = max(year))

#original script drops numerous smolt outmigrant series that apparently could not be well reconciled to FRAM units
smolt_full_orig <- readr::read_csv(fp$coho_file, show_col_types = FALSE) |> 
  mutate(
    pop = case_when(
      !is.na(`SaSI Population`) ~ `SaSI Population`,
      is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Managment Unit (FRAM)`,
      is.na(`SaSI Population`) & !is.na(SubPopulation) ~ SubPopulation
    )
  ) |> 
  select(1:8, pop) |> 
  #count(Smolt_Abundance_Matches_FRAM)
  #count(Smolt_Abunce_Matches_SASI_but_not_FRAM)
  filter(!is.na(`Smolt Abundance`), `Calendar Year` >= 1986)
#see "drop_pop"
smolt_used |> count(pop, smolt_ocn_surv_pop) #9 units
smolt_full_orig |> count(`Managment Unit (FRAM)`, `Smolt Abundance Population`, pop) |> print(n = 50) #23 units

left_join(
  smolt_used |> count(pop, smolt_ocn_surv_pop)
  ,
  smolt_full_orig |> count(`Managment Unit (FRAM)`, `Smolt Abundance Population`, pop) #|> print(n = 50)
  , by = c("pop" = "Managment Unit (FRAM)")
  ) |> 
  filter(!(pop.y %in% drop_pop)) |> 
  select(-contains("."))

```

```{r smolt_update_checks_export, eval=FALSE}

smolt <- readxl::read_excel("O:/code/coho/forecast_wb/Smolt Time Series.xlsx", na = "NA") |> 
  select(
    year = OEY,
    Chehalis_149 = Chehalis,
    Deschutes_63 = Deschutes,
    Dungeness_107 = Dungeness,
    Green_97 = Green,
    Nisqually_69 = Nisqually,
    Nooksack_1 = Nooksack,
    Queets_139 = Queets.Clear,
    Skagit_17 = Skagit,
    Snohomish_35 = Snohomish) |> 
  filter(year >= 1986) |> 
  pivot_longer(cols = -year, names_to = "smoltpop_StockID", values_to = "smolt_update") |> 
  separate(smoltpop_StockID, into = c("smoltpop","StockID"), sep = "_") |> 
  mutate(StockID = as.numeric(StockID)) |> 
  filter(!is.na(smolt_update)) #excel wide format has complete cases, so reduce to stock-years with data


full_join(
  smolt_used |> select(pop_id, pop, StockID, year, smolt_orig = smolt)
  ,
  smolt
  ,
  by = c("StockID", "year")
  ) |> #214
  mutate(
    smolt = if_else(is.na(smolt_update), smolt_orig, smolt_update)
  ) |> 
  #filter(is.na(smolt_orig)) #16 updated values for 2018-2020
  #filter(round(smolt_orig) != round(smolt_update)) #1991 Chehalis does not match
  #filter(is.na(smolt_update)) #prior year vals not present in update, 3 Chehalis and 1 Nooksack
  select(StockID, year, smolt) |> 
  write_csv("data/smolt_outmigrants.csv")

```

## Marine survival CWT releases and recoveries

Dataset updated by Neala Kendall and Ty Garber, following fix of RMIS release count error for tagged wild units. Prior dataset was unaffected because 1) error not present in hatchery units 2) previous wild unit values drawn from underlying pre-RMIS data. 

```{r cwt_rec_rel_update_checks_export, eval=FALSE}
# #strings match and all 15 units used in analysis are present
# #reversing order of terms shows the 3 unused dataset units: Baker H, Minter Crk H and Satsop H
# setdiff(
#   filter(pop_meta, !is.na(smolt_ocn_surv_pop)) |> distinct(smolt_ocn_surv_pop),
#   readxl::read_excel("O:/code/coho/forecast_wb/coho SARs_to extend.xlsx",
#     sheet = "CWT_FRAM_Matches_complete202108") |>
#     distinct(smolt_ocn_surv_pop = `Smolt Ocean Survival Population`)
# )

cwt <- readxl::read_excel(
  "O:/code/coho/forecast_wb/coho SARs_to extend.xlsx", 
  sheet = "CWT_FRAM_Matches_complete202108"
  ) |> 
  select(
    smolt_ocn_surv_pop = `Smolt Ocean Survival Population`,
    pop = `Managment Unit (FRAM)`,
    year = `Calendar Year`, est_n_rec = Fishery_Plus_Escapement, est_n_rel = Release_No
  ) |> 
  left_join(
    pop_meta |> mutate(pop_id = as.numeric(factor(pop))),
    by = c("pop", "smolt_ocn_surv_pop")
    ) |> 
  #filter(is.na(pop_id)) |> print(n = 100)
  #filter(str_detect(pop, "Area 13")) |> print(n = 100)
  filter(!is.na(pop_id)) |> 
  select(StockID, pop_id, pop, smolt_ocn_surv_pop, year, est_n_rec, est_n_rel)

# #compare against prior
# tbl_coho |> 
#   filter(pop_id == 36, between(year, 1986, 2020)) |> 
#   select(pop_id, pop, `Smolt Ocean Survival Population`, year, Fishery_Plus_Escapement, Release_No) |> print(n = 50)

# full_join(
#   coho_data_tbl |> select(pop_id, pop, smolt_ocn_surv_pop, year, est_n_rec, est_n_rel)
#   ,
#   cwt
#   ,
#   by = c("pop_id", "pop", "smolt_ocn_surv_pop", "year"), suffix = c("_orig", "")
#   ) |>
#   #filter(pop_id == 34, between(year, 1986, 2020)) |> print(n = 50)
#   mutate(
#     d_rec = est_n_rec - est_n_rec_orig,
#     d_rel = est_n_rel - est_n_rel_orig
#   ) |>
#   filter(
#     !is.na(d_rec), !is.na(d_rel),
#     (d_rec != 0 | d_rel != 0)) |>
#   select(pop_id:year, contains("_rel"), contains("_rec")) |> #print(n = 200)
#   writexl::write_xlsx("O:/code/coho/forecast_wb/coho_SARs_to_extend_diffs.xlsx")

cwt |> 
  filter(year >= 1986) |>
  select(StockID, year, est_n_rec, est_n_rel) |> 
  write_csv("data/rmis_cwts.csv")

```

## full dataset

```{r tbl_coho_full, eval=FALSE}
# #still retaining for now to avoid having to root through commits for any questions
# 
# drop_pop <- c("Queets", "Clearwater", 'Bell Creek', 'Johnson Creek', 'Jimmy Come Lately Creek',
#               'Deep Creek', 'McDonald Creek', 'Siebert Creek', 'Salt Creek', 'Discovery Bay',
#               'East Twin Creek', 'West Twin Creek', 'Northeast Hood Canal')
# 
# drop_cwt_surv <- c("Minter Crk H", "Baker H", "Satsop H") 
# 
# tbl_coho <- readr::read_csv(fp$coho_file, show_col_types = FALSE) |> #1709x21
#   mutate(
#     pop = case_when(
#       !is.na(`SaSI Population`) ~ `SaSI Population`,
#       is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Managment Unit (FRAM)`,
#       is.na(`SaSI Population`) & !is.na(SubPopulation) ~ SubPopulation
#     ),
#     spwn = case_when(
#       !is.na(`SaSI Population`) ~ `SASI Natural Origin Abundance`,
#       is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Age 3 Escapement (FRAM)`,
#       is.na(`SaSI Population`) & !is.na(SubPopulation) ~ `SupPopulation Escapement`
#     ),
#     spwn = if_else(pop=="Discovery Bay", `SASI CompositeOrigin Abundance`, abs(spwn)) #corrects negative val for 2005 A12A Wild, orig L152
#     ,
#     hvst = spwn / (1/`Harvest (% FRAM)` - 1) 
#   ) |> 
#   filter( !(pop %in% drop_pop) ) |> 
#   select(`Calendar Year`, `Smolt Abundance`, Latitude, Longitude, pop, spwn, hvst) |> 
#   inner_join(
#     readr::read_csv(fp$stream_file, show_col_types = FALSE) |> 
#       filter(Population != "", !is.na(Population)) |> #no NAs in the current csv, but left for now
#       select(pop = Population, KM)
#     ,
#     by = "pop") |> 
#   full_join(
#     readr::read_csv(fp$cwt_file, show_col_types = FALSE) |> 
#       mutate(
#         hat = if_else(stringr::str_detect(`Smolt Ocean Survival Population`, " H$"), 1, 0)
#       ) |> 
#       filter( !(`Smolt Ocean Survival Population` %in% drop_cwt_surv)) |> 
#       select(pop = `Managment Unit (FRAM)`, `Calendar Year`, `Smolt Ocean Survival Population`, Fishery_Plus_Escapement, Release_No, hat)
#     ,
#     by = c("pop", "Calendar Year")) |> 
#   filter(`Calendar Year` > 1985) |> 
#   bind_rows(
#     tibble(`Calendar Year` = 2013, yr = 28, pop = "Area 7-7A Independent Wild"),
#     tibble(`Calendar Year` = 2004, yr = 19, pop = "Port Gamble Bay Wild"),
#     tibble(`Calendar Year` = 2000:2001, yr = 15:16, pop = "Grays Harbor Miscellaneous Wild")
#   ) |>
#   left_join(
#     bind_rows(
#       tibble(pop = "Green River Wild", Long = -122.2145, Lat = 47.3519),
#       tibble(pop = "Area 10E Miscellaneous Wild", Long = -122.8242, Lat = 47.5896)
#     )
#     , by = "pop") |>
#   mutate(
#     Longitude = if_else(is.na(Long), Longitude, Long), Long = NULL,
#     Latitude = if_else(is.na(Lat), Latitude, Lat), Lat = NULL,
#     Basin = if_else(Longitude < -123.80, 0, 1),
#     `Smolt Abundance` = if_else(pop == "Puyallup River Wild", NA_real_, `Smolt Abundance`),
#     hvst = if_else(stringr::str_detect(pop, "Quillayute River") & `Calendar Year` <= 1987, NA_real_, hvst),
#     pop_id = as.numeric(factor(pop)),
#     yr = `Calendar Year` - min(`Calendar Year`) + 1
#   ) |> 
#   rename(year = `Calendar Year`) |> 
#   arrange(pop)
# 
# sf_coord <- tbl_coho |> 
#   group_by(pop, pop_id) |> 
#   summarise(long = median(Longitude, na.rm = T), lat = median(Latitude, na.rm = T), .groups = "drop") |> 
#   sf::st_as_sf(coords = c("long", 'lat'), crs = sf::st_crs("+proj=longlat +datum=WGS84")) |> 
#   sf::st_transform(crs = sf::st_crs("+proj=utm +zone=10T ellps=WGS84"))

```

```{r updating_full_dataset, eval=FALSE}
#Note the original negative escapement in A12A Wild in 2005 is due to an error in the FRAM database
#so still needs correcting: spwn is coerced to 0 here, but harvest is unaffected b/c drawn directly from Mortality

#here the meta identifiers are (expanding left) joined to the FRAM data
#after these have been full joined to full joined CWT and smolt data

coho_data_tbl <- pop_meta |>
  full_join(
    full_join( #FRAM + (CWT + smolt)
      read_csv(fp$data_fram) |>
        select(-StockLongName) |>
        mutate(
          spwn = if_else(spwn < 0, 0.01, spwn),
          rtrn = spwn + hvst
        ) #1153, 1986:2019
      ,
      full_join(
        read_csv(fp$data_cwt), #396
        read_csv(fp$data_smolt), #214 
        by = c("StockID", "year")
        )
      ,
      by = c("StockID", "year")
      )
    ,
    by = c("StockID"))

write_csv(coho_data_tbl, "data/fram_cwt_smolt_fulljoin.csv")

# coho_data_tbl |> count(StockID, pop_id, pop) |> print(n = 100)
# 
# #WB has FRAM through 2019, hatchery-based rel/rec through 2018, no smolt
# coho_data_tbl |> filter(StockID == 161) |> print(n = 50)
# 
# #Chehalis has the Bingham Crk numbers
# coho_data_tbl |> filter(StockID == 149) |> print(n = 50)
# 
# #Skagit is an example of "max data": FRAM to 2019, CWT to 2018, smolt to 2020
# coho_data_tbl |> filter(StockID == 17) |> print(n = 50)

```

\

# model fitting

The STIPM and AR1 stan models are first repeatedly fit to progressive subsets of the data filtered by year in order to gauge *as applied* forecast skill. Rather than a "sliding window" (e.g., the prior 10 years stepped forward each year), the dataset is "stretched" from a fixed starting year. Although older observations may have less immediate relevance to current conditions, this approach allows the MCMC sampler to explore a larger parameter space for posterior distributions. Future work could compare skill with a sliding alternative to test the hypothesis that a more constrained but more recent span of years could improve prediction.

In practice, the data available to forecast a given year return may be limited to observations from one, two or even three years prior. These lags can vary by data type and source.

For example, a 2022 preseason forecast, made in January of 2022, would likely have available:
  - Smolt outmigrant abundances from 2020 or 2021
  - CWT recoveries & releases (informing marine survival) from 2019 or 2020; from 2017 brood, released in 2019 with recoveries in 2020
  - FRAM-based estimates of harvest and escapement (informing annual total return) from 2019 or 2020
    - 2019 values are complete from the previous preseason planning process
    - 2020 estimates exist and are being compiled for Jan/Feb CoTC post-season runs
    - preliminary 2021 escapement estimates may be available, but will still be in review and undergoing regional QAQC
    
Accordingly, the process here examines 2 likely dataset scenarios for a forecast targeting year $y$:
  - STIPM: $smolt_{y-1}$, $MS_{y-2}$, $spwn_{y-2}, hvst_{y-2}$ or $spwn_{y-3}, hvst_{y-3}$
  - AR1: $spwn_{y-2}, hvst_{y-2}$ or $spwn_{y-3}, hvst_{y-3}$

## stan functions

A helper function trims years from the complete dataset according to arguments controlling the data-type-specific lag from a data `year_max` corresponding to the year before the desired forecast year (i.e., a `year_max` of 2021 designates the most recent possible data for the 2022 forecast). 

```{r stan_data_filter}
#trim years then declare separate intermediaries
#"year_max" is "last/max year of available data", so desired year_pred-1
#lags allow for flexible OAT configurations with additional trimming
#but can be zeroed to allow all available data
#for a 2022 forecast in adult_pred vectors from stan
#year_max is 2021, s.t. firmly available postseason FRAM is 2019 
stan_data_filter <- function(
  full, #complete dataset
  year_min, #starting anchor year
  #in STIPM, adult_pred[year_max+1] is a total return from smolt[year_max]*surv[year_max], with adult_est[year_max] as the spawning escapement
  #in AR1, adult_est and adult_pred are total return, and adult_pred[year_pred] is same as last year of adult_est b/c passing n_year+1 relative to STIPM
  year_max,
  lag_smolt = 0, #n-extra-years trimmed from smolt abundances 
  lag_MS = 1, #n-extra-years trimmed from release and recovery data
  lag_spwn = 2, #n-extra-years trimmed from FRAM escapement
  lag_hvst = 2 #n-extra-years trimmed from FRAM harvest
  ){
  
  full_ymin_ymax = filter(full, between(year, year_min, year_max)) |> 
    mutate(yr = year - min(year) + 1)
  
  stan_data = list(
    y_min_max = year_min:year_max,
    
    # smolt = filter(full_ymin_ymax, !is.na(`Smolt Abundance`), year <= year_max - lag_smolt),
    # MS = filter(full_ymin_ymax, !is.na(Release_No), year <= year_max - lag_MS),
    smolt = filter(full_ymin_ymax, !is.na(smolt), year <= year_max - lag_smolt),
    MS = filter(full_ymin_ymax, !is.na(est_n_rec), year <= year_max - lag_MS),
    
    spwn = filter(full_ymin_ymax, !is.na(spwn), year <= year_max - lag_spwn),
    hvst = filter(full_ymin_ymax, !is.na(hvst), year <= year_max - lag_hvst) |> 
      mutate(hvst = abs(hvst)),
    rtrn = full_ymin_ymax |> 
      #mutate(rtrn = spwn + abs(hvst)) |>
      filter(!is.na(rtrn), year <= year_max - max(c(lag_spwn, lag_hvst)))
  )
  
  return(stan_data)
}

# #tests, random start year
# stan_data_filter(coho_data_tbl, 2004, 2006) |> map(tail) #predicting 2007, with defaults only get FRAM through 2004
# stan_data_filter(coho_data_tbl, 2004, 2007) |> map(tail) #predicting 2008, defaults give FRAM 2005

```

Next, wrappers to the `rstan::stan` function facilitate repeated fitting with consistent control parameters and input data lists.

```{r ar1_functions}
stan_ar1 <- function(stan_data, n_iter = 200, n_chain = 2){
  #note following orig naming convention where "tot" refers to "total return" = spwn + hvst = rtrn, as estimated from FRAM
  stan_fit <- stan(
    file = 'LD_coho_forecast_AR_ind_2.stan',
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.99, max_treedepth = 10.25),
    data = list(
      n_year = length(stan_data$y_min_max) + 1, #sets year of adult_pred and year-dim of adult_est 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      n_pop_tot = length(unique(stan_data$rtrn$pop_id)), #Number of populations with return data
      pop_tot = unique(stan_data$rtrn$pop_id), #Which populations possess return data
      n_tot = length(stan_data$rtrn$rtrn), #Length of the return data vectors
      tot_dat = stan_data$rtrn$rtrn,  #Vectors of all return data across all populations
      tot_true = stan_data$rtrn$yr, #Vectors of the indices identifying which years are those with non-NA data for the return data
      #Paired vectors of slice points indicating the beginning, and end of the data for a particular population
      slice_tot_start = stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_tot_end = stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  
  return(stan_fit)
}

```

**Could get `stream_dist` from LU or sf_coord? Other fixed values here? As above, could avoid call to global `tbl_coho` for n_pop**

```{r stipm_functions}
stan_stipm <- function(stan_data, n_iter = 200, n_chain = 2){
  #note n_year does not include "+1" of AR1 since adult_pred is calc'd separately as
  # smolt[n_year]*surv[n_year], thereby giving adult run size (spwn+hvst) in n_year+1
  #stan_data$y_min_max accordingly is first:last_year_of_data
  stan_fit <- stan(
    file = 'LD_coho_forecast_6_2_4.stan', 
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.99, max_treedepth = 10.25),
    data = list(
      n_year = length(stan_data$y_min_max), 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      u = matrix(1, nrow = 1, ncol = length(unique(stan_data$spwn$pop_id))),
      dist = units::drop_units(sf::st_distance(sf_coord)/10000), #values are identical
      
      pop_smolt = unique(stan_data$smolt$pop_id), #pop_ids with smolt data 
      n_pop_smolt = length(unique(stan_data$smolt$pop_id)),
      smolt_true = stan_data$smolt$yr,
      smolt_dat = stan_data$smolt$smolt,
      n_smolt = nrow(stan_data$smolt),
      
      pop_esc = unique(stan_data$spwn$pop_id),  # pop_ids with escapement data
      n_pop_esc = length(unique(stan_data$spwn$pop_id)),
      esc_true = stan_data$spwn$yr,
      esc_dat = stan_data$spwn$spwn,
      n_esc = nrow(stan_data$spwn),
      
      pop_catch = unique(stan_data$hvst$pop_id), #pop_ids with harvest data
      n_pop_catch = length(unique(stan_data$hvst$pop_id)),
      harvest_true = stan_data$hvst$yr,
      harvest_dat = stan_data$hvst$hvst,
      n_harvest = nrow(stan_data$hvst),
      
      pop_MS = unique(stan_data$MS$pop_id), #pop_ids with marine survival data
      n_pop_MS = length(unique(stan_data$MS$pop_id)),
      MS_true = stan_data$MS$yr, #orig uses Fishery_Plus_Escapement rather than Release_No to filter...
      MS_dat_x = stan_data$MS$est_n_rec |> round() |> as.integer(), #stan expects integer; previously labeled Fishery_Plus_Escapement
      MS_dat_N = stan_data$MS$est_n_rel |> round() |> as.integer(), #previously labeled Release_No
      n_MS = nrow(stan_data$MS),
      
      stream_dist = stan_data$spwn |> distinct(pop, hab_km) |> pluck("hab_km"),
      
      sigma_esc = 0.2,
      
      n_hatchery = filter(stan_data$MS, hat == 1) |> distinct(pop_id) |> nrow(),
      hatchery = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat > 0, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"),
      wild = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat < 1, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"), 
      
      slice_smolt_start = stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_smolt_end = stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_esc_start = stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_esc_end = stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_harvest_start = stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_harvest_end = stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_MS_start = stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_MS_end = stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  return(stan_fit)
}
```

## oat

```{r oat, eval=FALSE}
#verbose/redundant chunk, but explicit _lag2 and _lag1 for certainty/clarity
#intermediary single year output is saved out within map() in case of loop disruptions

#a dataset year_max of 2014, predicting 2015, with default lags of 2 gives FRAM/spwn through 2012
#but with "compiling" lags of 1 gives FRAM/spwn through 2013

oat_wrap <- function(iter, #3K 
                     chain, #4
                     oat_year_min, #1998,
                     oat_years_max, #2008:2018, #max data year, predicting 2009:2019
                     lag_spwn, lag_hvst, lag_MS, lag_smolt #deducted from data year
                     ){
  #build a list named by max data year 
  oat <- set_names(oat_years_max) |> 
    map(function(x) {
      print(paste("max data year", x, "predicting", x+1))
      stan_data_list <- stan_data_filter(coho_data_tbl, year_min = oat_year_min, year_max = x, lag_spwn = lag_spwn, lag_hvst = lag_hvst, lag_MS = lag_MS, lag_smolt = lag_smolt) 
      print(stan_data_list$y_min_max)
      print(c("smolt", range(stan_data_list$smolt$year)))
      print(c("MS", range(stan_data_list$MS$year)))
      print(c("spwn", range(stan_data_list$spwn$year)))
      print(c("hvst", range(stan_data_list$hvst$year)))
      print(c("rtrn", range(stan_data_list$rtrn$year)))

      print(paste("AR1 start", Sys.time()))

      fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall AR1 stan data arg n_year+1 relative to STIPM gives adult_pred in desired year
      fit_ar1_pred <- summary(fit_ar1, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "ar1",
          n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      print(paste("STIPM start", Sys.time()))

      fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall stipm adult_pred is smolt[n_year]*surv[n_year] to give adult run size (spwn+hvst) n_year+1
      fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "stipm",
          n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      pred_out <- bind_rows(fit_ar1_pred, fit_stipm_pred)
      saveRDS(pred_out, paste0("oat_fits/oat_pred_",x+1,"_lags_spwn",lag_spwn,"_hvst",lag_hvst,"_MS",lag_MS,"_smolt",lag_smolt,".rds"))
      return(pred_out) #list element for max-data-year-x 
    })
  return(oat)
}

#predicting 2009:2019
oat_lag2 <- oat_wrap(iter = 3000, chain = 4, oat_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 2, lag_hvst = 2, lag_MS = 1, lag_smolt = 0)
oat_lag1 <- oat_wrap(iter = 3000, chain = 4, oat_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 1, lag_hvst = 1, lag_MS = 1, lag_smolt = 0)
oat_lag0 <- oat_wrap(iter = 3000, chain = 4, oat_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 0, lag_hvst = 0, lag_MS = 1, lag_smolt = 0)


# oat <- bind_rows(
#   bind_rows(oat_lag2) |> mutate(lag = "l2")
#   ,
#   bind_rows(oat_lag1) |> mutate(lag = "l1")
# )
# 
# saveRDS(oat, "oat_fits/oat_l2_l1_2010_2018.rds")

```

## REVISE full dataset

Needs review 

```{r full_data}
iter <- 200
chain <- 2
year_min <- 1998
year_max <- 2018

stan_data_list <- stan_data_filter(coho_data_tbl, year_min = year_min, year_max = year_max, lag_spwn = 1, lag_hvst = 1) 

print(stan_data_list$y_min_max)
print(c("smolt", range(stan_data_list$smolt$year)))
print(c("MS", range(stan_data_list$MS$year)))
print(c("spwn", range(stan_data_list$spwn$year)))
print(c("hvst", range(stan_data_list$hvst$year)))
print(c("rtrn", range(stan_data_list$rtrn$year)))

fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = iter, n_chain = chain)

# fit_ar1_pred <- 
summary(fit_ar1, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #should be x+1
    mod = "ar1",
    n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |> 
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = iter, n_chain = chain)

fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #should be x+1
    mod = "stipm",
    n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |> 
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

```


# results

```{r read_oat}
#tibbles of annual predictions for AR1 & STIPM with a given lag 

bind_rows(
  list.files("oat_fits", pattern = "lags_spwn0_hvst0", full.names = T) |>
    map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l0_", mod))
  ,
  list.files("oat_fits", pattern = "lags_spwn1_hvst1", full.names = T) |> 
    map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l1_", mod))
  ,
  list.files("oat_fits", pattern = "lags_spwn2_hvst2", full.names = T) |> 
    map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l2_", mod))
) |> 
  select(lag_mod, year, StockID, pop_id, pop, n_diverg, n_eff, Rhat, `2.5%`:`97.5%`) |> 
  saveRDS("oat_fits/oat_l0_l1_l2_2009_2019.rds")

oat <- readRDS("oat_fits/oat_l0_l1_l2_2009_2019.rds")

oat |> group_by(lag_mod, year) |> 
  summarise(
    n_diverg = max(n_diverg),
    Rhat_med = median(Rhat), Rhat_max = max(Rhat),
    n_eff_med = median(n_eff), n_eff_min = min(n_eff),
    .groups = "drop") |> 
  print(n = 80)

```

```{r oat_obs, eval=FALSE}
(
oat_obs <- left_join(
  oat,
  coho_data_tbl |> select(year, StockID, rtrn) |> 
    #adding MASE denominator
    group_by(StockID) |> 
    mutate(
      scale_err = mean(abs(rtrn - c(NA, head(rtrn, -1))), na.rm = T)
    ) |> 
    ungroup(),
  by = c("year", "StockID")
  ) |>
  select(lag_mod, StockID, pop_id, pop, year, scale_err, rtrn,  `2.5%`:`97.5%`) |> 
  mutate(
    in_50 = (rtrn >= `25%`) & (rtrn <= `75%`),
    in_95 = (rtrn >= `2.5%`) & (rtrn <= `97.5%`),
    err = rtrn - `50%`, #positive is underforecast, negative is "fewer than expected"
    err_abs = abs(err),
    err_log = log(rtrn) - log(`50%`),
    err_pct = err / rtrn,
    err_abs_pct = err_abs / rtrn,
    #kept as Eq26 in paper with pred-obs
    mase = abs(`50%` - rtrn) / scale_err
  )
)
```



## diagnostics

Add N-divergent

```{r oat_rhat_neff}
# oat |> 
#   group_by(lag_mod, pop_id, pop) |> 
#   summarise(Rhat_max = max(Rhat), n_eff_min = min(n_eff), .groups = "drop") |> 
#   pivot_wider(names_from = lag_mod, values_from = c(Rhat_max, n_eff_min)) |>
#   select(pop_id, pop, starts_with("Rhat"), starts_with("n_eff")) |> print(n = 40)

# #all years
# oat |> 
#   select(year, lag_mod, pop_id, pop, Rhat, n_eff) |> 
#   filter(pop_id == 34) |> 
#   #pivot_longer(-c(year, lag_mod, pop_id, pop), names_to = "var", values_to = "val") |> 
#   gt::gt(groupname_col = "year", rowname_col = "lag_mod") |> 
#   gt::cols_hide(c("pop_id", "pop")) |> 
#   gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "Willapa Bay Natural") |> 
#   gt::fmt_number(Rhat, decimals = 2) |> gt::fmt_number(n_eff, decimals = 0)

#max/min
oat |>
  group_by(lag_mod, pop_id, pop) |>
  summarise(Rhat_max = max(Rhat), n_eff_min = min(n_eff), .groups = "drop") |>
  filter(pop_id == 34) |> 
  gt::gt(rowname_col = "lag_mod") |> 
  gt::cols_hide(c("pop_id", "pop")) |> 
  gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "Willapa Bay Natural") |> 
  gt::fmt_number(Rhat_max, decimals = 2) |> gt::fmt_number(n_eff_min, decimals = 0)

```


## performance

```{r oat_obs_mase_by_lag}
#MASE ratios by lag
oat_obs |> 
  select(pop_id, pop, lag_mod, year, mase) |>
  pivot_wider(names_from = lag_mod, values_from = mase) |> 
  mutate(
    l1_l2_ar1 = l1_ar1 / l2_ar1,
    l1_l2_stipm = l1_stipm / l2_stipm,
    l1_stipm_ar1 = l1_stipm / l1_ar1
  ) |> 
  # group_by(pop_id, pop) |> 
  # summarise(across(starts_with("l1_l2"), list(min = ~min(.), med = ~median(.), max = ~max(.)))) |> print(n = 50)
  filter(pop_id == 34) |>
  select(pop, year, l1_ar1, l2_ar1, l1_l2_ar1, l1_stipm, l2_stipm, l1_l2_stipm, l1_stipm_ar1) |> 
  gt() |> 
  fmt_number(l1_ar1:l1_stipm_ar1, decimals = 2) |> 
  tab_spanner("AR1", l1_ar1:l1_l2_ar1) |> 
  tab_spanner("STIPM", contains("stipm"))


```

```{r oat_obs_l1_vs_l2_stipm}
# #WB focus
# coho_data_tbl |> filter(pop_id == 34) |> select(pop_id:StockID, year, spwn, hvst, rtrn) |> print(n = 50)
# oat_obs |> filter(pop_id == 34) |> filter(str_detect(lag_mod, "stipm"))

#compare lags for STIPM
oat_obs |> 
  select(pop_id, lag_mod, year, rtrn:`90%`, err:mase, in_10_90) |>
  filter(str_detect(lag_mod, "stipm")) |> arrange(lag_mod, year) |> 
  filter(pop_id == 34) |>
  #gt(groupname_col = "year", rowname_col = "lag_mod")
  gt(groupname_col = "lag_mod", rowname_col = "year") |> 
  cols_hide(pop_id) |> 
  fmt_number(rtrn:err_abs, decimals = 0) |> 
  fmt_number(err_log:mase, decimals = 2) |> 
  summary_rows(groups = TRUE, fns = list(min = ~min(.), med = ~median(.), max = ~max(.)))
```

```{r oat_obs_ar1_vs_stipm_l1}
#compare STIPM vs AR1 lag1    
oat_obs |> 
  select(pop_id, lag_mod, year, rtrn:`90%`, err:mase, in_10_90) |>
  filter(str_detect(lag_mod, "l1")) |> arrange(lag_mod, year) |> 
  filter(pop_id == 34) |>
  #gt(groupname_col = "year", rowname_col = "lag_mod")
  gt(groupname_col = "lag_mod", rowname_col = "year") |> 
  cols_hide(pop_id) |> 
  fmt_number(rtrn:err_abs, decimals = 0) |> 
  fmt_number(err_log:mase, decimals = 2) |> 
  summary_rows(groups = TRUE, fns = list(min = ~min(.), med = ~median(.), max = ~max(.)))
```


```{r odds_and_ends}
# rr <- bind_cols(
#   readxl::read_excel("O:/code/coho/forecast_wb/2021 WB4 Coho Forecast Model DRAFT 12.14.2020.xlsx", range = "RR!C38:C48", col_names = "year"),
#   readxl::read_excel("O:/code/coho/forecast_wb/2021 WB4 Coho Forecast Model DRAFT 12.14.2020.xlsx", range = "RR!E38:E48", col_names = "escp"),
#   readxl::read_excel("O:/code/coho/forecast_wb/2021 WB4 Coho Forecast Model DRAFT 12.14.2020.xlsx", range = "RR!U38:U48", col_names = "catch")) |> 
#   mutate(trs = escp + catch)



#all years, all pops
oat_obs |> 
  group_by(lag_mod) |> 
  summarise(
    n = n(),
    in_10_90_sum = sum(in_10_90),
    in_10_90_pct = in_10_90_sum / n,
    across(err:mase, median),
    .groups = "drop")

#all years by pop
oat_obs |> 
  group_by(lag_mod, pop_id, pop) |> 
  summarise(
    n = n(),
    in_10_90_sum = sum(in_10_90),
    in_10_90_pct = in_10_90_sum / n,
    across(err:mase, median),
    .groups = "drop") |> 
  filter(pop_id == 34)

# #values are terminal run sizes from regional workbook
# #catches do not include preterm, so escapements line up but t9$obs != stipm$rtrn 
# t9 <- readxl::read_excel("O:/code/coho/forecast_wb/table9_2020_submission.xlsx") |> 
#   mutate(
#     err = obs - mar_surv,
#     err_abs = abs(err),
#     err_log = log(obs) - log(mar_surv),
#     err_abs_pct = err_abs / obs
#   )
# 
# t9 |> filter(between(year, 2013, 2018)) |> summarise(across(err:err_abs_pct, median))
# oat_obs |> arrange(mod, year) |> 
#   filter(pop_id == 34) |> select(mod, year, rtrn, `10%`:`90%`, err:err_abs_pct) |> group_by(mod) |> summarise(across(err:err_abs_pct, median))

#the large returns in 2009 & 2010 seem to predispose overforecasts in 17 & 18
#ar1 bad but better than stipm in 2015
oat_obs |> 
  filter(pop_id > 30) |> 
  filter(pop_id == 34) |> 
  ggplot(aes(x = year, fill = lag_mod, color = lag_mod)) +
  scale_y_continuous("Return", labels = scales::comma) +
  wacolors::scale_fill_wa_d(aesthetics = c("fill", "color")) +
  geom_ribbon(aes(ymin = `10%`, ymax = `90%`), alpha = 0.2) + 
  geom_line(aes(y = `50%`)) +
  geom_point(aes(y = rtrn), color = 1)  +
  facet_wrap(~pop+lag_mod, scales = "free")
```

## forecasts

```{r oat_obs_pred_patchwork}
set_names(2009:2017) |> 
  map(function(x) {
    d <- coho_data_tbl |> filter(pop_id == 34, between(year, 1998, x)) |> select(year, pop_id, rtrn) |> mutate(year = as.character(year))
    
    oat_obs |> 
      filter(pop_id == 34, str_detect(lag_mod, "l1"), year == x+1) |> mutate(year = as.character(year)) |> 
      ggplot() +
      geom_pointrange(aes(x = year, y = `50%`, ymin = `10%`, ymax = `90%`, color = lag_mod), fatten = 1.1, position = position_dodge(width = 0.5)) +
      geom_point(aes(x = year, y = `50%`, color = lag_mod), position = position_dodge(width = 0.5)) +
      geom_point(aes(x = year, y = rtrn), color = 1, shape = 2)  +
      #geom_line(data = d, aes(x = year, y = rtrn), color = "brown") +
      geom_point(data = d, aes(x = year, y = rtrn), color = "brown", shape = 16) +
      geom_vline(xintercept = as.character(x-1), linetype = "dashed") +
      scale_x_discrete(name = "", limits = as.character(1998:2018), drop = FALSE) +
      scale_y_continuous(name = "Return", limits = c(0, 150000), labels = scales::comma) + #limits drops point error when outside ymax
      scale_color_manual(name = "", values = c("grey", "gold"))
}) |> patchwork::wrap_plots(ncol = 1)
```

```{r oat_obs_pred_gganimate}
animate(
  set_names(2009:2017) |> 
    map_df(function(x) {
      full_join(
        coho_data_tbl |> 
          filter(pop_id == 34, between(year, 1998, x)) |> 
          select(year, pop_id, train = rtrn) |> 
          mutate(year = as.character(year))
        ,
        oat_obs |> 
          filter(pop_id == 34, year == x+1, str_detect(lag_mod, "l1")) |> 
          select(year, pop_id, pop, lag_mod, rtrn:`90%`) |> 
          mutate(year = as.character(year))
        , 
        by = c("pop_id", "year")
      ) |> 
        mutate(year_pred = x+1, year_data = as.character(x-1))
    }) |> 
    ggplot(aes(x = year)) +
    geom_pointrange(aes(y = `50%`, ymin = `10%`, ymax = `90%`, color = lag_mod), position = position_dodge2(width = 0.5), size = 1.2) +
    geom_point(aes(y = `50%`, color = lag_mod), position = position_dodge2(width = 0.5), size = 1.5) +
    geom_point(aes(y = rtrn), color = "tan", shape = 15, size = 4)  +
    #  geom_point(aes(y = train), color = "brown", shape = 16) +
    geom_col(aes(y = train), color = "tan", fill = "tan", width = 0.3, alpha = 0.7) +
   # geom_vline(aes(xintercept = year_data), linetype = "dashed") +
    scale_x_discrete(name = "", limits = as.character(1998:2018), drop = FALSE, guide = guide_axis(n.dodge = 2)) +
    scale_y_continuous(name = "Return", limits = c(0, 130000), labels = scales::comma) + #limits drops point error when outside ymax
    scale_color_manual(name = "", values = c("grey", "gold"), na.translate = FALSE) + 
    theme(legend.position = "top") +
    transition_states(year_pred, 3, 1) #+ shadow_mark(alpha = 0.4)
  ,
  width = 7, height = 5, units = "in", res = 100
  )

anim_save("oat_l1_anim.gif")

```


# Can delete soon

## LU export

Dump a file for discussion and review of included series.

```{r stipm_pops, eval=FALSE}
# tbl_coho |> 
#   group_by(pop_id, pop) |> 
#   summarise(
#     year_min = min(`Calendar Year`),
#     year_max = max(`Calendar Year`),
#     nobs_any = n(),
#     across(c(spwn, hvst, `Smolt Abundance`, Release_No), ~sum(!is.na(.))),
#     .groups = "drop"
#   ) |> #print(n = 50)
#   ##writexl::write_xlsx("stipm_pops_crosscheck.xlsx")

# #4 cases of missing KM/Lat/Long due to the orig sequence of dataset construction
# #where a few additional records are added after joining stream lengths
# #but expect to drop 7/7A and Pt Gamble anyway
# #also following orig approach on "median" of Lat/Long, though already uniform 
# #manually copied into working xlsx, leaving pop_id/pops to confirm matches before deleting
# tbl_coho |> 
#   filter(!is.na(KM)) |>
#   group_by(pop_id, pop) |> 
#   summarise(across(c(Longitude, Latitude, KM), median), .groups = "drop") |> 
#   write.table("clipboard", row.names = F)

# # add CWT marine survival info
# tbl_coho |> 
#   distinct(pop_id, pop,`Smolt Ocean Survival Population`, hat) |> 
#   filter(!is.na(hat)) |> 
#   select(pop_id, `Smolt Ocean Survival Population`, hat) |> 
#   right_join(tibble(pop_id = 1:36), by = "pop_id") |> arrange(pop_id) |> 
#   write.table("clipboard", row.names = F, na = "")

```

```{r try_tidybayes, eval=FALSE}
#could further examine library(tidybayes)? and/or library(bayesplot)?
library(tidybayes)

fit_ar1 |> 
  recover_types(tbl_coho_stan$rtrn) |> #not doing much since pops already numerically indexed
  spread_draws(adult_est[y,i]) |> #TMI?
  median_qi()
```

```{r demo_stretch_allpops, eval=FALSE}
library(slider)

#stretching: before = Inf, so cumulative
foo <- tbl_coho_stan$rtrn |> 
  #filter( pop_id %in% focal_popids) |> 
  select(year, pop_id, pop, rtrn) |>
  group_by(pop_id, pop) |> 
  mutate(
    rtrn_mu_cml = slide_dbl(rtrn, mean, .before = Inf),
    lm_fit = slide(
      .x = cur_data(), #the subset of rows
      .f = ~lm(rtrn ~ year, data = .x), #the thing to do to them
      .before = Inf, #how far back to look
      .complete = T
    ),
    lm_pred = lm_fit |> 
      map_dbl(~if_else(is.null(.x), NA_real_, last(.x$fitted.values)))
  ) |> 
  ungroup()

foo |> 
  select(-lm_fit) |> 
  filter(pop_id > 30) |> #reduce for plotting 
  pivot_longer(cols = -c(year, pop_id, pop), names_to = "var", values_to = "val") |> 
  {\(x) 
  ggplot(x, aes(year, val, color = var)) +
  geom_line(data = filter(x, var != "rtrn")) +
  geom_point(data = filter(x, var == "rtrn")) + 
  scale_x_continuous(n.breaks = 10,  guide = guide_axis(n.dodge = 2)) +
  facet_wrap(~pop_id, scales = "free", ncol = 1)
  }()
```

```{r test_2050s, eval=FALSE}
#further into future just with larger n_year? oh yes.

fit_ar1_test <- stan(
  file = 'LD_coho_forecast_AR_ind_2.stan',
  iter = 200, chains = 2, thin = 1, seed = 222,
  control = list(adapt_delta = 0.99, max_treedepth = 10.25),
  data = list(
    #Number of years (total, includes several missing years for some stocks)
    n_year = length(unique(tbl_coho$year))*2,
    #Number of total populations    
    n_pop = length(unique(tbl_coho$pop_id)),
    #Number of populations with return data
    n_pop_tot = length(unique(tbl_coho_stan$rtrn$pop_id)),
    #Which populations possess return data
    pop_tot = unique(tbl_coho_stan$rtrn$pop_id),
    #Length of the return data vectors
    n_tot = nrow(tbl_coho_stan$rtrn), #length(tot_dat)
    #Vectors of all return data across all populations
    tot_dat = tbl_coho_stan$rtrn$rtrn,
    #Vectors of the indices identifying which years are those with non-NA data for the return data
    tot_true = tbl_coho_stan$rtrn$yr,
    #Paired vectors of slice points indicating the beginning, and end of the data for a particular population
    slice_tot_start = tbl_coho_stan$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
    slice_tot_end = tbl_coho_stan$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )

fit_ar1_test_smry <- fit_smry(fit_ar1_test)

fit_ar1_test_smry$adult_est |> #tail() 
  filter(pop_id > 30) |> 
  ggplot(aes(x = year)) +
  scale_y_continuous("Estimated adult return", labels = scales::comma) +
  scale_fill_brewer(type = "qual", aesthetics = c("fill", "color") ) +
  geom_ribbon(aes(ymin = `10%`, ymax = `90%`), alpha = 0.2) + 
  geom_line(aes(y = `50%`)) +
  geom_point(
    data = tbl_coho_stan$rtrn |> filter(pop_id > 30) |> select(year, pop, pop_id, rtrn),
    aes(y = rtrn),
    inherit.aes = T) +
  facet_wrap(~pop)

```

